[
  {
    "objectID": "01-geodemographics.html",
    "href": "01-geodemographics.html",
    "title": "Geodemographics",
    "section": "",
    "text": "This week we will see how we can use socio-demographic and socio-economic data to characterise neighbourhoods using geodemographics. Geodemographics is the “analysis of people by where they live (Harris et al. 2005) and as such entails representing the individual and collective identities that are manifest in observable neighbourhood structure” (Longley 2012). We will look at geodemographics by focusing on an existing geodemographic classification known as the Internet User Classification.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. A. 2012. Geodemographics and the practices of geographic information science. International Journal of Geographical Information Science 26(12): 2227-2237. [Link]\nMartin, D., Gale, C., Cockings, S. et al. 2018. Origin-destination geodemographics for analysis of travel to work flows. Computers, Environment and Urban Systems 67: 68-79. [Link]\nSingleton, A., Alexiou, A. and Savani, R. 2020. Mapping the geodemographics of digital inequality in Great Britain: An integration of machine learning into small area estimation. Computers, Environment and Urban Systems 82: 101486. [Link]\nWyszomierski, J., Longley, P. A., Singleton, A. et al. 2023. A neighbourhood Output Area Classification from the 2021 and 2022 UK censuses. The Geographical Journal. Online First. [Link]\n\n\n\n\n\nFränti, P., Sieronoja, S. 2019. How much can k-means be improved by using better initialization and repeats? Pattern Recognition 93: 95-112. [Link]\nGoodman, A., Wilkinson, P., Stafford, M. et al. 2011. Characterising socio-economic inequalities in exposure to air pollution: A comparison of socio-economic markers and scales of measurement. Health & Place 17(3): 767-774. [Link]\nSingleton, A. and Spielman, S. 2014. The past, present, and future of geodemographic research in the United States and United Kingdom. The Professional Geographer 66(4): 558-567. [Link]\n\n\n\n\n\nThe CDRC Internet User Classification (IUC) is a bespoke geodemographic classification that describes how people residing in different parts of Great Britain interact with the Internet. For every Lower Super Output Area (LSOA) in England and Wales and Data Zone (DZ) (2011 Census Geographies), the IUC provides aggregate population estimates of Internet use and provides insights into the geography of the digital divide in the United Kingdom:\n\n“Digital inequality is observable where access to online resources and those opportunities that these create are non-egalitarian. As a result of variable rates of access and use of the Internet between social and spatial groups (..), this leads to digital differentiation, which entrenches difference and reciprocates digital inequality over time” (Singleton et al. 2020).\n\n\n\nFor the first part of this week’s practical material, we will be looking at the Internet User Classification (IUC) for Great Britain in more detail by mapping it.\nOur first step is to download the IUC data set:\n\nOpen a web browser and go to the data portal of the CDRC.\nRegister if you need to, or if you are already registered, make sure you are logged in.\nSearch for Internet User Classification.\nScroll down and choose the download option for the IUC 2018 (CSV).\nSave the iuc_gb_2018.csv file in an appropriate folder.\n\n\n\n\n\n\nFigure 1: Download the GB IUC 2018\n\n\n\n\nStart by inspecting the data set in MS Excel, or any other spreadsheet software such as Apache OpenOffice Calc or Numbers. Also, have a look at the IUC 2018 User Guide that provides the pen portraits of every cluster, including plots of cluster centres and a brief summary of the methodology.\n\n\n\n\n\n\nIt is always a good idea to inspect your data prior to analysis to find out how your data look like. Of course, depending on the type of data, you can choose any tool you like to do this inspection (ArcGIS, R, QGIS, Microsoft Excel, etc.).\n\n\n\n\n\n\n\n\nFigure 2: GB IUC 2018 in Excel\n\n\n\n\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(tmap)\n\n# load data\niuc &lt;- read_csv(\"data/index/iuc-gb-2018.csv\")\n\n# inspect\niuc\n\n\n# A tibble: 41,729 × 5\n   SHP_ID LSOA11_CD LSOA11_NM              GRP_CD GRP_LABEL                    \n    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;                        \n 1      1 E01020179 South Hams 012C             5 e-Rational Utilitarians      \n 2      2 E01033289 Cornwall 007E               9 Settled Offline Communities  \n 3      3 W01000189 Conwy 015F                  5 e-Rational Utilitarians      \n 4      4 W01001022 Bridgend 014B               7 Passive and Uncommitted Users\n 5      5 W01000532 Ceredigion 007B             9 Settled Offline Communities  \n 6      6 E01018888 Cornwall 071G               9 Settled Offline Communities  \n 7      7 E01018766 Cornwall 028D               9 Settled Offline Communities  \n 8      8 E01019948 East Devon 010C             9 Settled Offline Communities  \n 9      9 W01000539 Ceredigion 005D             5 e-Rational Utilitarians      \n10     10 E01019171 Barrow-in-Furness 005E      6 e-Mainstream                 \n# ℹ 41,719 more rows\n\n\n\n\n\nR code\n\n# inspect data types\nstr(iuc)\n\n\nspc_tbl_ [41,729 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SHP_ID   : num [1:41729] 1 2 3 4 5 6 7 8 9 10 ...\n $ LSOA11_CD: chr [1:41729] \"E01020179\" \"E01033289\" \"W01000189\" \"W01001022\" ...\n $ LSOA11_NM: chr [1:41729] \"South Hams 012C\" \"Cornwall 007E\" \"Conwy 015F\" \"Bridgend 014B\" ...\n $ GRP_CD   : num [1:41729] 5 9 5 7 9 9 9 9 5 6 ...\n $ GRP_LABEL: chr [1:41729] \"e-Rational Utilitarians\" \"Settled Offline Communities\" \"e-Rational Utilitarians\" \"Passive and Uncommitted Users\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SHP_ID = col_double(),\n  ..   LSOA11_CD = col_character(),\n  ..   LSOA11_NM = col_character(),\n  ..   GRP_CD = col_double(),\n  ..   GRP_LABEL = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nNow the data are loaded we can move to acquiring our spatial data. As the IUC is created at the level of the Lower layer Super Output Area Census geography, we need to download its administrative borders. As the data set for the entire country is quite large, we will focus on Liverpool.\n\nGo to the UK Data Service Census support portal and select Boundary Data Selector.\nSet Country to England, Geography to Statistical Building Block, dates to 2011 and later, and click Find.\nSelect English Lower Layer Super Output Areas, 2011 and click List Areas.\nSelect Liverpool from the list and click Extract Boundary Data.\nWait until loaded and download the BoundaryData.zip file.\nUnzip and save the file.\n\n\n\n\n\n\n\nYou could also have downloaded the shapefile with the data already joined to the LSOA boundaries directly from the CDRC data portal, but this is the national data set and is quite large (75MB). Also, as we will be looking at Liverpool today we do not need all LSOAs in Great Britain..\n\n\n\nNow we got the administrative boundary data, we can prepare the IUC map by joining our csv file with the IUC classification to the shapefile.\n\n\n\nR code\n\n# load libraries\nlibrary(sf)\nlibrary(tmap)\n\n# load spatial data\nliverpool &lt;- st_read(\"data/boundaries/england_lsoa_2011.shp\")\n\n\nReading layer `england_lsoa_2011' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114Q/data/boundaries/england_lsoa_2011.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\ntm_shape(liverpool) + tm_polygons()\n\n\n\n\nFigure 3: LSOAs Liverpool\n\n\n\n\n\n\n\nR code\n\n# join data\nliv_iuc &lt;- left_join(liverpool, iuc, by = c(code = \"LSOA11_CD\"))\n\n# inspect\nliv_iuc\n\n\nSimple feature collection with 298 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                         label           name      code SHP_ID      LSOA11_NM\n1  E08000012E02006934E01033755 Liverpool 062D E01033755  25097 Liverpool 062D\n2  E08000012E02006932E01033758 Liverpool 060B E01033758  24070 Liverpool 060B\n3  E08000012E02001356E01033759 Liverpool 010F E01033759  26845 Liverpool 010F\n4  E08000012E02006932E01033762 Liverpool 060E E01033762  26866 Liverpool 060E\n5  E08000012E02001396E01032505 Liverpool 050F E01032505  27848 Liverpool 050F\n6  E08000012E02001396E01032506 Liverpool 050G E01032506   2429 Liverpool 050G\n7  E08000012E02001396E01032507 Liverpool 050H E01032507  24242 Liverpool 050H\n8  E08000012E02001373E01032508 Liverpool 027G E01032508  28413 Liverpool 027G\n9  E08000012E02001373E01032509 Liverpool 027H E01032509  24339 Liverpool 027H\n10 E08000012E02001354E01032510 Liverpool 008F E01032510  25167 Liverpool 008F\n   GRP_CD                     GRP_LABEL                       geometry\n1       2               e-Professionals POLYGON ((334276.7 391012.8...\n2       4         Youthful Urban Fringe POLYGON ((335723 391178, 33...\n3       7 Passive and Uncommitted Users POLYGON ((338925 394476, 33...\n4       1           e-Cultural Creators POLYGON ((334612.4 391111.7...\n5       7 Passive and Uncommitted Users POLYGON ((335894.7 387448.3...\n6       6                  e-Mainstream POLYGON ((336256.7 387691.8...\n7       3                    e-Veterans POLYGON ((336803.5 387432.7...\n8      10                   e-Withdrawn POLYGON ((339299 391470, 33...\n9       7 Passive and Uncommitted Users POLYGON ((338901 391308, 33...\n10      7 Passive and Uncommitted Users POLYGON ((338018.2 395716.4...\n\n\n\n\n\nR code\n\n# inspect\ntm_shape(liv_iuc) + tm_fill(col = \"GRP_LABEL\") + tm_layout(legend.outside = TRUE)\n\n\n\n\n\nFigure 4: Internet User Classification Liverpool\n\n\n\n\nLet’s use the same colours as used on CDRC mapmaker by specifying the hex colour codes for each of our groups. Note the order of the colours is important: the colour for group 1 is first, group 2 second and so on.\n\n\n\nR code\n\n# define palette\niuc_colours &lt;- c(\"#dd7cdc\", \"#ea4d78\", \"#d2d1ab\", \"#f36d5a\", \"#a5cfbc\", \"#e4a5d0\",\n    \"#8470ff\", \"#79cdcd\", \"#808fee\", \"#ffd39b\")\n\n# plot pretty\ntm_shape(liv_iuc) + tm_fill(col = \"GRP_LABEL\", palette = iuc_colours) + tm_layout(legend.outside = TRUE)\n\n\n\n\n\nFigure 5: Internet User Classification Liverpool with mapmaker colours\n\n\n\n\n\n\n\nNow we have these cluster classifications, how can we link them to people? Try using the Mid-Year Population Estimates 2019 that you can download below to:\n\ncalculate the total number of people associated with each cluster group for England and Wales as a whole; and\ncreate a pretty data visualisation showing the results (no map!).\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLSOA-level Mid-Year Population Estimates England and Wales 2019\ncsv\nDownload\n\n\nLower-layer Super Output Areas Great Britain 2011\nshp\nDownload\n\n\n\n\n\n\n\nIn several cases, including the 2011 residential-based area classifications and the Internet User Classification, a technique called k-means clustering is used in the creation of a geodemographic classification. K-means clustering aims to partition a set of observations into a number of clusters (k), in which each observation will be assigned to the cluster with the nearest mean. As such, a cluster refers to a collection of data points aggregated together because of certain similarities (i.e. standardised scores of your input data). In order to run a k-means clustering, you first define a target number k of clusters that you want. The k-means algorithm subsequently assigns every observation to one of the clusters by finding the solution that minimises the total within-cluster variance. For the second part of this week’s practical material, we will be replicating part of the Internet User Classification for Great Britain. For this we will be using an MSOA-level input data set containing various socio-demographic and socio-economic variables that you can download below together with the MSOA administrative boundaries.\nThe data set contains the following variables:\n\n\n\nVariable\nDefinition\n\n\n\n\nmsoa11cd\nMSOA Code\n\n\nage_total, age0to4pc, age5to14pc, age16to24pc, age25to44pc, age45to64pc, age75pluspc\nPercentage of people in various age groups\n\n\nnssec_total, 1_higher_managerial, 2_lower_managerial, 3_intermediate_occupations, 4_employers_small_org, 5_lower_supervisory, 6_semi_routine, 7_routine, 8_unemployed\nPercentage of people in selected operational categories and sub-categories classes drawn from the National Statistics Socio-economic Classification (NS-SEC)\n\n\navg_dwn_speed, avb_superfast, no_decent_bband, bband_speed_under2mbs, bband_speed_under10mbs, bband_speed_over30mbs\nMeasures of broadband use and internet availability\n\n\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nMiddle-layer Super Output Areas Great Britain 2011\nshp\nDownload\n\n\nMSOA-level input variables for IUC\ncsv\nDownload\n\n\n\n\n\n\nR code\n\n# load data\niuc_input &lt;- read_csv(\"data/index/msoa-iuc-input.csv\")\n\n# inspect\nhead(iuc_input)\n\n\n# A tibble: 6 × 23\n  msoa11cd  age_total age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 E02000001      7375    0.032      0.0388      0.0961       0.407       0.273\n2 E02000002      6775    0.0927     0.122       0.113        0.280       0.186\n3 E02000003     10045    0.0829     0.102       0.118        0.306       0.225\n4 E02000004      6182    0.0590     0.102       0.139        0.254       0.250\n5 E02000005      8562    0.0930     0.119       0.119        0.299       0.214\n6 E02000007      8791    0.103      0.125       0.129        0.285       0.197\n# ℹ 16 more variables: age75pluspc &lt;dbl&gt;, nssec_total &lt;dbl&gt;,\n#   `1_higher_managerial` &lt;dbl&gt;, `2_lower_managerial` &lt;dbl&gt;,\n#   `3_intermediate_occupations` &lt;dbl&gt;, `4_employers_small_org` &lt;dbl&gt;,\n#   `5_lower_supervisory` &lt;dbl&gt;, `6_semi_routine` &lt;dbl&gt;, `7_routine` &lt;dbl&gt;,\n#   `8_unemployed` &lt;dbl&gt;, avg_dwn_speed &lt;dbl&gt;, avb_superfast &lt;dbl&gt;,\n#   no_decent_bband &lt;dbl&gt;, bband_speed_under2mbs &lt;dbl&gt;,\n#   bband_speed_under10mbs &lt;dbl&gt;, bband_speed_over30mbs &lt;dbl&gt;\n\n\nBefore running our k-means clustering algorithm, we need to extract the data which we want to use; i.e. we need to remove all the columns with data that we do not want to include in the clustering process.\n\n\n\nR code\n\n# column names\nnames(iuc_input)\n\n\n [1] \"msoa11cd\"                   \"age_total\"                 \n [3] \"age0to4pc\"                  \"age5to14pc\"                \n [5] \"age16to24pc\"                \"age25to44pc\"               \n [7] \"age45to64pc\"                \"age75pluspc\"               \n [9] \"nssec_total\"                \"1_higher_managerial\"       \n[11] \"2_lower_managerial\"         \"3_intermediate_occupations\"\n[13] \"4_employers_small_org\"      \"5_lower_supervisory\"       \n[15] \"6_semi_routine\"             \"7_routine\"                 \n[17] \"8_unemployed\"               \"avg_dwn_speed\"             \n[19] \"avb_superfast\"              \"no_decent_bband\"           \n[21] \"bband_speed_under2mbs\"      \"bband_speed_under10mbs\"    \n[23] \"bband_speed_over30mbs\"     \n\n# extract columns by index\ncluster_data &lt;- iuc_input[, c(3:8, 10:17, 18:20)]\n\n# inspect\nhead(cluster_data)\n\n# A tibble: 6 × 17\n  age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    0.032      0.0388      0.0961       0.407       0.273      0.0607\n2    0.0927     0.122       0.113        0.280       0.186      0.0980\n3    0.0829     0.102       0.118        0.306       0.225      0.0646\n4    0.0590     0.102       0.139        0.254       0.250      0.0886\n5    0.0930     0.119       0.119        0.299       0.214      0.0501\n6    0.103      0.125       0.129        0.285       0.197      0.0688\n# ℹ 11 more variables: `1_higher_managerial` &lt;dbl&gt;, `2_lower_managerial` &lt;dbl&gt;,\n#   `3_intermediate_occupations` &lt;dbl&gt;, `4_employers_small_org` &lt;dbl&gt;,\n#   `5_lower_supervisory` &lt;dbl&gt;, `6_semi_routine` &lt;dbl&gt;, `7_routine` &lt;dbl&gt;,\n#   `8_unemployed` &lt;dbl&gt;, avg_dwn_speed &lt;dbl&gt;, avb_superfast &lt;dbl&gt;,\n#   no_decent_bband &lt;dbl&gt;\n\n\nWe also need to rescale the data so all input data are presented on a comparable scale: the average download speed data (i.e. avg_dwn_speed) is very different to the other data that, for instance, represent the percentage of the population by different age groups.\n\n\n\nR code\n\n# rescale\ncluster_data &lt;- scale(cluster_data)\n\n# inspect\nhead(cluster_data)\n\n\n      age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n[1,] -1.7579913 -2.8680309 -0.36823669   2.1768529   0.2561260  -0.6039803\n[2,]  1.9519376  1.6403191 -0.05766949   0.1671857  -1.5639740   0.6215590\n[3,]  1.3549320  0.5475394  0.02888699   0.5773908  -0.7424464  -0.4769094\n[4,] -0.1050147  0.5938501  0.40759191  -0.2402277  -0.2247116   0.3136111\n[5,]  1.9687596  1.5173858  0.05639359   0.4733916  -0.9785969  -0.9539525\n[6,]  2.6064366  1.8434062  0.22116720   0.2379171  -1.3168781  -0.3384050\n     1_higher_managerial 2_lower_managerial 3_intermediate_occupations\n[1,]           4.4185012          1.8391416                 -2.2291104\n[2,]          -0.8510113         -0.9272602                  0.1031344\n[3,]          -0.4499409         -0.3266302                  1.5123844\n[4,]          -0.6741298         -0.3469851                  1.8104058\n[5,]          -0.9705088         -1.0065195                  0.5809449\n[6,]          -0.8571773         -1.0674213                 -0.1494470\n     4_employers_small_org 5_lower_supervisory 6_semi_routine   7_routine\n[1,]           -1.02298662         -2.53130431    -2.49888739 -1.81145433\n[2,]            0.39745656         -0.67859270     1.05051667  0.08877827\n[3,]            0.28308676         -0.37987493    -0.05595897 -0.52802287\n[4,]            0.13591535          0.24587683    -0.15093353 -0.07008362\n[5,]            0.09942212          0.42532220     0.76747039  0.35318453\n[6,]           -0.18786941          0.05518013     0.56442659  0.57156979\n     8_unemployed avg_dwn_speed avb_superfast no_decent_bband\n[1,]   -0.5139854    -1.6561653    -5.2186970      -0.3797659\n[2,]    1.1777334     0.7915882     0.5093876      -0.2031415\n[3,]    0.7615830     0.6354463     0.5222597      -0.3797659\n[4,]    0.2898589     0.9477301     0.5480039      -0.2472976\n[5,]    0.8482656     0.4701196     0.2648177       0.2384195\n[6,]    1.5510655     0.6813704     0.4965155      -0.1589854\n\n\nNow our data are all on the same scale, we will start by creating an elbow plot. The elbow method is a visual aid that can help in determining the number of clusters in a data set. Remember: this is important because with a k-means clustering you need to specify the numbers of clusters a priori!\nThe elbow method can help as it plots the total explained variation (‘Within Sum of Squares’) in your data as a function of the number of cluster. The idea is that you pick the number of clusters at the ‘elbow’ of the curve as this is the point in which the additional variation that would be explained by an additional cluster is decreasing. Effectively this means you are actually running the k-means clustering multiple times before running the actual k-means clustering algorithm.\n\n\n\nR code\n\n# create empty list to store the within sum of square values\nwss_values &lt;- list()\n\n# execute a k-means clustering for k=1, k=2, ..., k=15\nfor (i in 1:15) {\n    wss_values[i] &lt;- sum(kmeans(cluster_data, centers = i, iter.max = 30)$withinss)\n}\n\n# inspect\nwss_values\n\n\n[[1]]\n[1] 144143\n\n[[2]]\n[1] 110934.1\n\n[[3]]\n[1] 100042.6\n\n[[4]]\n[1] 82701.63\n\n[[5]]\n[1] 73974.93\n\n [ reached getOption(\"max.print\") -- omitted 10 entries ]\n\n\n\n\n\nR code\n\n# vector to dataframe\nwss_values &lt;- as.data.frame(wss_values)\n\n# transpose\nwss_values &lt;- as.data.frame(t(wss_values))\n\n# add cluster numbers\nwss_values$cluster &lt;- seq.int(nrow(wss_values))\nnames(wss_values) &lt;- c(\"wss\", \"cluster\")\n\n# plot using ggplot2\nggplot(data = wss_values, aes(x = cluster, y = wss)) +\n  geom_point() +\n  geom_path() +\n  scale_x_continuous(breaks = seq(1, 15)) +\n  xlab(\"Number of clusters\") +\n  ylab(\"Within sum of squares\") +\n  theme_minimal()\n\n\n\n\n\nFigure 6: Within sum of squares by number of clusters\n\n\n\n\nBased on the elbow plot, we can now choose the number of clusters and it looks like 7 clusters would be a reasonable choice.\n\n\n\n\n\n\nThe interpretation of an elbow plot can be quite subjective and often multiple options would be justified: 6, 8, and perhaps 9 clusters also do not look unreasonable. You would need to try the different options and see what output you get to determine the ‘optimal’ solution. However, at very least, the elbow plot does give you an idea of what would potentially be an adequate number of clusters.\n\n\n\nNow we have decided on the number of clusters (i.e. 7 clusters), we can run our cluster analysis. We will be running this analysis 10 times because there is an element of randomness within the clustering, and we want to make sure we get the optimal clustering output.\n\n\n\nR code\n\n# create empty list to store the results of the clustering\nclusters &lt;- list()\n\n# create empty variable to store fit\nfit &lt;- NA\n\n# run the k-means 10 times\nfor (i in 1:10) {\n\n    # keep track of the runs\n    print(paste0(\"starting run: \", i))\n\n    # run the k-means clustering algorithm to extract 7 clusters\n    clust7 &lt;- kmeans(x = cluster_data, centers = 7, iter.max = 1e+06, nstart = 1)\n\n    # get the total within sum of squares for the run and\n    fit[i] &lt;- clust7$tot.withinss\n\n    # update the results of the clustering if the total within sum of squares\n    # for the run is lower than any of the runs that have been executed so far\n    if (fit[i] &lt; min(fit[1:(i - 1)])) {\n        clusters &lt;- clust7\n    }\n}\n\n\n[1] \"starting run: 1\"\n[1] \"starting run: 2\"\n[1] \"starting run: 3\"\n[1] \"starting run: 4\"\n[1] \"starting run: 5\"\n[1] \"starting run: 6\"\n[1] \"starting run: 7\"\n[1] \"starting run: 8\"\n[1] \"starting run: 9\"\n[1] \"starting run: 10\"\n\n# inspect\nclusters\n\nK-means clustering with 7 clusters of sizes 955, 2394, 1885, 759, 582, 187, 1718\n\nCluster means:\n   age0to4pc  age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n  1_higher_managerial 2_lower_managerial 3_intermediate_occupations\n  4_employers_small_org 5_lower_supervisory 6_semi_routine  7_routine\n  8_unemployed avg_dwn_speed avb_superfast no_decent_bband\n [ reached getOption(\"max.print\") -- omitted 7 rows ]\n\nClustering vector:\n[1] 4 1 2 2 1\n [ reached getOption(\"max.print\") -- omitted 8475 entries ]\n\nWithin cluster sum of squares by cluster:\n[1]  8146.662 14203.994 10426.288  7998.965  6828.975\n [ reached getOption(\"max.print\") -- omitted 2 entries ]\n (between_SS / total_SS =  56.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [ reached getOption(\"max.print\") -- omitted 4 entries ]\n\n\nWe now have to execute a bit of post-processing to extract some useful summary data for each cluster: the cluster size (size) and mean values for each cluster.\n\n\n\nR code\n\n# assign to new variable for clarity\nkfit &lt;- clusters\n\n# cluster sizes\nkfit_size &lt;- kfit$size\n\n# inspect\nkfit_size\n\n\n[1]  955 2394 1885  759  582\n [ reached getOption(\"max.print\") -- omitted 2 entries ]\n\n# mean values for each variable in each cluster\nkfit_mean &lt;- as_tibble(aggregate(cluster_data, by = list(kfit$cluster), FUN = mean))\nnames(kfit_mean)[1] &lt;- \"cluster\"\n\n# inspect\nkfit_mean\n\n# A tibble: 7 × 18\n  cluster age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n    &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1       1     1.68      1.22        0.401       0.719      -1.25      -1.03   \n2       2    -0.160    -0.0305     -0.201      -0.138       0.290      0.134  \n3       3     0.180     0.0509      0.0154     -0.0634     -0.0315    -0.00715\n4       4     0.351    -0.886       0.201       2.04       -1.22      -0.948  \n5       5    -0.891    -0.171      -0.520      -1.00        1.21       0.594  \n6       6    -1.52     -2.68        5.40        0.125      -2.62      -1.28   \n7       7    -0.596     0.0470     -0.460      -0.713       0.740      0.753  \n# ℹ 11 more variables: `1_higher_managerial` &lt;dbl&gt;, `2_lower_managerial` &lt;dbl&gt;,\n#   `3_intermediate_occupations` &lt;dbl&gt;, `4_employers_small_org` &lt;dbl&gt;,\n#   `5_lower_supervisory` &lt;dbl&gt;, `6_semi_routine` &lt;dbl&gt;, `7_routine` &lt;dbl&gt;,\n#   `8_unemployed` &lt;dbl&gt;, avg_dwn_speed &lt;dbl&gt;, avb_superfast &lt;dbl&gt;,\n#   no_decent_bband &lt;dbl&gt;\n\n\n\n\n\nR code\n\n# transform shape to tidy format\nkfit_mean_long &lt;- pivot_longer(kfit_mean, cols = (-cluster))\n\n# plot using ggplot2\nggplot(kfit_mean_long, aes(x = cluster, y = value, fill = name)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_x_continuous(breaks = seq(1, 7, by = 1)) +\n  xlab(\"Cluster\") +\n  ylab(\"Mean value\") +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\nFigure 7: Mean variable values by cluster\n\n\n\n\n\n\n\n\n\n\nYour values may be slightly different due to the random choice of initial cluster centres.\n\n\n\nLooking at the table with the mean values for each cluster and the barplot, we can see that variables contribute differently to the different clusters. The graph is a little busy, so you might want to look at the cluster groups or variables individually to get a better picture of each cluster.\n\n\n\nR code\n\n# plot using ggplot2\nggplot(kfit_mean_long[kfit_mean_long$cluster == 1, ], aes(x = cluster, y = value, fill = name)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  xlab(\"Cluster\") +\n  ylab(\"Mean value\") +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\nFigure 8: Mean variable values for cluster 1\n\n\n\n\nWe can also show the results of our geodemographic classification on a map.\n\n\n\nR code\n\n# read shape\nmsoa &lt;- st_read(\"data/boundaries/gb_msoa11_sim.shp\")\n\n\nReading layer `gb_msoa11_sim' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114Q/data/boundaries/gb_msoa11_sim.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8480 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5513 ymin: 5342.7 xmax: 655604.7 ymax: 1220302\nGeodetic CRS:  WGS 84\n\n# set projection\nst_crs(msoa) = 27700\n\n# simplify for speedier plotting\nmsoa &lt;- st_simplify(msoa, dTolerance = 100)\n\n# join\ncluster_data &lt;- cbind(iuc_input, kfit$cluster)\nmsoa &lt;- left_join(msoa, cluster_data, by = c(geo_code = \"msoa11cd\"))\n\n# plot\ntm_shape(msoa) + tm_fill(col = \"kfit$cluster\") + tm_layout(legend.outside = TRUE)\n\n\n\n\nFigure 9: Spatial pattern of our geodemographic classification\n\n\n\n\n\n\n\n\n\n\nYour map might look different as the cluster numbers are not consistently assigned and therefore colours can be map onto the cluster numbers differently.\n\n\n\n\n\n\n\nThe creation of a geodemographic classification is an iterative process of trial and error that involves the addition and removal of variables as well as experimentation with different numbers of clusters. It also might be, for instances, that some clusters are very focused on one group of data (e.g. age) and it could be an idea to group some ages together. If you want to make changes to the clustering solution, you can simply re-run the analysis with a different set of variables or with a different number of clusters by updating the code. However, it would be even simpler if you could automate some of the process.\nTry to create a R function that:\n\ntakes at least the following three arguments: a data frame that contains your input data, the number of clusters that you want, and a list of input variables;\nexecutes a k-means clustering on the input variables and the specified number of clusters; and,\nproduces a csv file that contains the table of means of the solution.\n\n\n\n\n\n\n\n\nYour function could look something like: run-kmeans(dataframe,number_of_clusters,input_variables)\nThe list of input variables does not have to be a list of names, but can also be a list containing the index values of the columns.\nHave a look at Hadley Wickhams explanation of functions in R.\n\n\n\n\n\n\n\n\nHaving finished this tutorial, you should now understand the basics of a geodemographic classification. In addition, you should have written a simple function. Although you have now reached the end of this week’s content, you could try and improve your function. Consider:\n\nIncluding maps or graphs in the code that get automatically saved.\nEnsuring that the csv outcome does not get overwritten every time you run you function.\nIncluding optional arguments in your function with default values if certain values are not specified."
  },
  {
    "objectID": "01-geodemographics.html#lecture-w07",
    "href": "01-geodemographics.html#lecture-w07",
    "title": "Geodemographics",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "01-geodemographics.html#reading-w07",
    "href": "01-geodemographics.html#reading-w07",
    "title": "Geodemographics",
    "section": "",
    "text": "Longley, P. A. 2012. Geodemographics and the practices of geographic information science. International Journal of Geographical Information Science 26(12): 2227-2237. [Link]\nMartin, D., Gale, C., Cockings, S. et al. 2018. Origin-destination geodemographics for analysis of travel to work flows. Computers, Environment and Urban Systems 67: 68-79. [Link]\nSingleton, A., Alexiou, A. and Savani, R. 2020. Mapping the geodemographics of digital inequality in Great Britain: An integration of machine learning into small area estimation. Computers, Environment and Urban Systems 82: 101486. [Link]\nWyszomierski, J., Longley, P. A., Singleton, A. et al. 2023. A neighbourhood Output Area Classification from the 2021 and 2022 UK censuses. The Geographical Journal. Online First. [Link]\n\n\n\n\n\nFränti, P., Sieronoja, S. 2019. How much can k-means be improved by using better initialization and repeats? Pattern Recognition 93: 95-112. [Link]\nGoodman, A., Wilkinson, P., Stafford, M. et al. 2011. Characterising socio-economic inequalities in exposure to air pollution: A comparison of socio-economic markers and scales of measurement. Health & Place 17(3): 767-774. [Link]\nSingleton, A. and Spielman, S. 2014. The past, present, and future of geodemographic research in the United States and United Kingdom. The Professional Geographer 66(4): 558-567. [Link]"
  },
  {
    "objectID": "01-geodemographics.html#geodemographics-1",
    "href": "01-geodemographics.html#geodemographics-1",
    "title": "Geodemographics",
    "section": "",
    "text": "The CDRC Internet User Classification (IUC) is a bespoke geodemographic classification that describes how people residing in different parts of Great Britain interact with the Internet. For every Lower Super Output Area (LSOA) in England and Wales and Data Zone (DZ) (2011 Census Geographies), the IUC provides aggregate population estimates of Internet use and provides insights into the geography of the digital divide in the United Kingdom:\n\n“Digital inequality is observable where access to online resources and those opportunities that these create are non-egalitarian. As a result of variable rates of access and use of the Internet between social and spatial groups (..), this leads to digital differentiation, which entrenches difference and reciprocates digital inequality over time” (Singleton et al. 2020).\n\n\n\nFor the first part of this week’s practical material, we will be looking at the Internet User Classification (IUC) for Great Britain in more detail by mapping it.\nOur first step is to download the IUC data set:\n\nOpen a web browser and go to the data portal of the CDRC.\nRegister if you need to, or if you are already registered, make sure you are logged in.\nSearch for Internet User Classification.\nScroll down and choose the download option for the IUC 2018 (CSV).\nSave the iuc_gb_2018.csv file in an appropriate folder.\n\n\n\n\n\n\nFigure 1: Download the GB IUC 2018\n\n\n\n\nStart by inspecting the data set in MS Excel, or any other spreadsheet software such as Apache OpenOffice Calc or Numbers. Also, have a look at the IUC 2018 User Guide that provides the pen portraits of every cluster, including plots of cluster centres and a brief summary of the methodology.\n\n\n\n\n\n\nIt is always a good idea to inspect your data prior to analysis to find out how your data look like. Of course, depending on the type of data, you can choose any tool you like to do this inspection (ArcGIS, R, QGIS, Microsoft Excel, etc.).\n\n\n\n\n\n\n\n\nFigure 2: GB IUC 2018 in Excel\n\n\n\n\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(tmap)\n\n# load data\niuc &lt;- read_csv(\"data/index/iuc-gb-2018.csv\")\n\n# inspect\niuc\n\n\n# A tibble: 41,729 × 5\n   SHP_ID LSOA11_CD LSOA11_NM              GRP_CD GRP_LABEL                    \n    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;                        \n 1      1 E01020179 South Hams 012C             5 e-Rational Utilitarians      \n 2      2 E01033289 Cornwall 007E               9 Settled Offline Communities  \n 3      3 W01000189 Conwy 015F                  5 e-Rational Utilitarians      \n 4      4 W01001022 Bridgend 014B               7 Passive and Uncommitted Users\n 5      5 W01000532 Ceredigion 007B             9 Settled Offline Communities  \n 6      6 E01018888 Cornwall 071G               9 Settled Offline Communities  \n 7      7 E01018766 Cornwall 028D               9 Settled Offline Communities  \n 8      8 E01019948 East Devon 010C             9 Settled Offline Communities  \n 9      9 W01000539 Ceredigion 005D             5 e-Rational Utilitarians      \n10     10 E01019171 Barrow-in-Furness 005E      6 e-Mainstream                 \n# ℹ 41,719 more rows\n\n\n\n\n\nR code\n\n# inspect data types\nstr(iuc)\n\n\nspc_tbl_ [41,729 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SHP_ID   : num [1:41729] 1 2 3 4 5 6 7 8 9 10 ...\n $ LSOA11_CD: chr [1:41729] \"E01020179\" \"E01033289\" \"W01000189\" \"W01001022\" ...\n $ LSOA11_NM: chr [1:41729] \"South Hams 012C\" \"Cornwall 007E\" \"Conwy 015F\" \"Bridgend 014B\" ...\n $ GRP_CD   : num [1:41729] 5 9 5 7 9 9 9 9 5 6 ...\n $ GRP_LABEL: chr [1:41729] \"e-Rational Utilitarians\" \"Settled Offline Communities\" \"e-Rational Utilitarians\" \"Passive and Uncommitted Users\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   SHP_ID = col_double(),\n  ..   LSOA11_CD = col_character(),\n  ..   LSOA11_NM = col_character(),\n  ..   GRP_CD = col_double(),\n  ..   GRP_LABEL = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nNow the data are loaded we can move to acquiring our spatial data. As the IUC is created at the level of the Lower layer Super Output Area Census geography, we need to download its administrative borders. As the data set for the entire country is quite large, we will focus on Liverpool.\n\nGo to the UK Data Service Census support portal and select Boundary Data Selector.\nSet Country to England, Geography to Statistical Building Block, dates to 2011 and later, and click Find.\nSelect English Lower Layer Super Output Areas, 2011 and click List Areas.\nSelect Liverpool from the list and click Extract Boundary Data.\nWait until loaded and download the BoundaryData.zip file.\nUnzip and save the file.\n\n\n\n\n\n\n\nYou could also have downloaded the shapefile with the data already joined to the LSOA boundaries directly from the CDRC data portal, but this is the national data set and is quite large (75MB). Also, as we will be looking at Liverpool today we do not need all LSOAs in Great Britain..\n\n\n\nNow we got the administrative boundary data, we can prepare the IUC map by joining our csv file with the IUC classification to the shapefile.\n\n\n\nR code\n\n# load libraries\nlibrary(sf)\nlibrary(tmap)\n\n# load spatial data\nliverpool &lt;- st_read(\"data/boundaries/england_lsoa_2011.shp\")\n\n\nReading layer `england_lsoa_2011' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114Q/data/boundaries/england_lsoa_2011.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\ntm_shape(liverpool) + tm_polygons()\n\n\n\n\nFigure 3: LSOAs Liverpool\n\n\n\n\n\n\n\nR code\n\n# join data\nliv_iuc &lt;- left_join(liverpool, iuc, by = c(code = \"LSOA11_CD\"))\n\n# inspect\nliv_iuc\n\n\nSimple feature collection with 298 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                         label           name      code SHP_ID      LSOA11_NM\n1  E08000012E02006934E01033755 Liverpool 062D E01033755  25097 Liverpool 062D\n2  E08000012E02006932E01033758 Liverpool 060B E01033758  24070 Liverpool 060B\n3  E08000012E02001356E01033759 Liverpool 010F E01033759  26845 Liverpool 010F\n4  E08000012E02006932E01033762 Liverpool 060E E01033762  26866 Liverpool 060E\n5  E08000012E02001396E01032505 Liverpool 050F E01032505  27848 Liverpool 050F\n6  E08000012E02001396E01032506 Liverpool 050G E01032506   2429 Liverpool 050G\n7  E08000012E02001396E01032507 Liverpool 050H E01032507  24242 Liverpool 050H\n8  E08000012E02001373E01032508 Liverpool 027G E01032508  28413 Liverpool 027G\n9  E08000012E02001373E01032509 Liverpool 027H E01032509  24339 Liverpool 027H\n10 E08000012E02001354E01032510 Liverpool 008F E01032510  25167 Liverpool 008F\n   GRP_CD                     GRP_LABEL                       geometry\n1       2               e-Professionals POLYGON ((334276.7 391012.8...\n2       4         Youthful Urban Fringe POLYGON ((335723 391178, 33...\n3       7 Passive and Uncommitted Users POLYGON ((338925 394476, 33...\n4       1           e-Cultural Creators POLYGON ((334612.4 391111.7...\n5       7 Passive and Uncommitted Users POLYGON ((335894.7 387448.3...\n6       6                  e-Mainstream POLYGON ((336256.7 387691.8...\n7       3                    e-Veterans POLYGON ((336803.5 387432.7...\n8      10                   e-Withdrawn POLYGON ((339299 391470, 33...\n9       7 Passive and Uncommitted Users POLYGON ((338901 391308, 33...\n10      7 Passive and Uncommitted Users POLYGON ((338018.2 395716.4...\n\n\n\n\n\nR code\n\n# inspect\ntm_shape(liv_iuc) + tm_fill(col = \"GRP_LABEL\") + tm_layout(legend.outside = TRUE)\n\n\n\n\n\nFigure 4: Internet User Classification Liverpool\n\n\n\n\nLet’s use the same colours as used on CDRC mapmaker by specifying the hex colour codes for each of our groups. Note the order of the colours is important: the colour for group 1 is first, group 2 second and so on.\n\n\n\nR code\n\n# define palette\niuc_colours &lt;- c(\"#dd7cdc\", \"#ea4d78\", \"#d2d1ab\", \"#f36d5a\", \"#a5cfbc\", \"#e4a5d0\",\n    \"#8470ff\", \"#79cdcd\", \"#808fee\", \"#ffd39b\")\n\n# plot pretty\ntm_shape(liv_iuc) + tm_fill(col = \"GRP_LABEL\", palette = iuc_colours) + tm_layout(legend.outside = TRUE)\n\n\n\n\n\nFigure 5: Internet User Classification Liverpool with mapmaker colours\n\n\n\n\n\n\n\nNow we have these cluster classifications, how can we link them to people? Try using the Mid-Year Population Estimates 2019 that you can download below to:\n\ncalculate the total number of people associated with each cluster group for England and Wales as a whole; and\ncreate a pretty data visualisation showing the results (no map!).\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLSOA-level Mid-Year Population Estimates England and Wales 2019\ncsv\nDownload\n\n\nLower-layer Super Output Areas Great Britain 2011\nshp\nDownload\n\n\n\n\n\n\n\nIn several cases, including the 2011 residential-based area classifications and the Internet User Classification, a technique called k-means clustering is used in the creation of a geodemographic classification. K-means clustering aims to partition a set of observations into a number of clusters (k), in which each observation will be assigned to the cluster with the nearest mean. As such, a cluster refers to a collection of data points aggregated together because of certain similarities (i.e. standardised scores of your input data). In order to run a k-means clustering, you first define a target number k of clusters that you want. The k-means algorithm subsequently assigns every observation to one of the clusters by finding the solution that minimises the total within-cluster variance. For the second part of this week’s practical material, we will be replicating part of the Internet User Classification for Great Britain. For this we will be using an MSOA-level input data set containing various socio-demographic and socio-economic variables that you can download below together with the MSOA administrative boundaries.\nThe data set contains the following variables:\n\n\n\nVariable\nDefinition\n\n\n\n\nmsoa11cd\nMSOA Code\n\n\nage_total, age0to4pc, age5to14pc, age16to24pc, age25to44pc, age45to64pc, age75pluspc\nPercentage of people in various age groups\n\n\nnssec_total, 1_higher_managerial, 2_lower_managerial, 3_intermediate_occupations, 4_employers_small_org, 5_lower_supervisory, 6_semi_routine, 7_routine, 8_unemployed\nPercentage of people in selected operational categories and sub-categories classes drawn from the National Statistics Socio-economic Classification (NS-SEC)\n\n\navg_dwn_speed, avb_superfast, no_decent_bband, bband_speed_under2mbs, bband_speed_under10mbs, bband_speed_over30mbs\nMeasures of broadband use and internet availability\n\n\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nMiddle-layer Super Output Areas Great Britain 2011\nshp\nDownload\n\n\nMSOA-level input variables for IUC\ncsv\nDownload\n\n\n\n\n\n\nR code\n\n# load data\niuc_input &lt;- read_csv(\"data/index/msoa-iuc-input.csv\")\n\n# inspect\nhead(iuc_input)\n\n\n# A tibble: 6 × 23\n  msoa11cd  age_total age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 E02000001      7375    0.032      0.0388      0.0961       0.407       0.273\n2 E02000002      6775    0.0927     0.122       0.113        0.280       0.186\n3 E02000003     10045    0.0829     0.102       0.118        0.306       0.225\n4 E02000004      6182    0.0590     0.102       0.139        0.254       0.250\n5 E02000005      8562    0.0930     0.119       0.119        0.299       0.214\n6 E02000007      8791    0.103      0.125       0.129        0.285       0.197\n# ℹ 16 more variables: age75pluspc &lt;dbl&gt;, nssec_total &lt;dbl&gt;,\n#   `1_higher_managerial` &lt;dbl&gt;, `2_lower_managerial` &lt;dbl&gt;,\n#   `3_intermediate_occupations` &lt;dbl&gt;, `4_employers_small_org` &lt;dbl&gt;,\n#   `5_lower_supervisory` &lt;dbl&gt;, `6_semi_routine` &lt;dbl&gt;, `7_routine` &lt;dbl&gt;,\n#   `8_unemployed` &lt;dbl&gt;, avg_dwn_speed &lt;dbl&gt;, avb_superfast &lt;dbl&gt;,\n#   no_decent_bband &lt;dbl&gt;, bband_speed_under2mbs &lt;dbl&gt;,\n#   bband_speed_under10mbs &lt;dbl&gt;, bband_speed_over30mbs &lt;dbl&gt;\n\n\nBefore running our k-means clustering algorithm, we need to extract the data which we want to use; i.e. we need to remove all the columns with data that we do not want to include in the clustering process.\n\n\n\nR code\n\n# column names\nnames(iuc_input)\n\n\n [1] \"msoa11cd\"                   \"age_total\"                 \n [3] \"age0to4pc\"                  \"age5to14pc\"                \n [5] \"age16to24pc\"                \"age25to44pc\"               \n [7] \"age45to64pc\"                \"age75pluspc\"               \n [9] \"nssec_total\"                \"1_higher_managerial\"       \n[11] \"2_lower_managerial\"         \"3_intermediate_occupations\"\n[13] \"4_employers_small_org\"      \"5_lower_supervisory\"       \n[15] \"6_semi_routine\"             \"7_routine\"                 \n[17] \"8_unemployed\"               \"avg_dwn_speed\"             \n[19] \"avb_superfast\"              \"no_decent_bband\"           \n[21] \"bband_speed_under2mbs\"      \"bband_speed_under10mbs\"    \n[23] \"bband_speed_over30mbs\"     \n\n# extract columns by index\ncluster_data &lt;- iuc_input[, c(3:8, 10:17, 18:20)]\n\n# inspect\nhead(cluster_data)\n\n# A tibble: 6 × 17\n  age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    0.032      0.0388      0.0961       0.407       0.273      0.0607\n2    0.0927     0.122       0.113        0.280       0.186      0.0980\n3    0.0829     0.102       0.118        0.306       0.225      0.0646\n4    0.0590     0.102       0.139        0.254       0.250      0.0886\n5    0.0930     0.119       0.119        0.299       0.214      0.0501\n6    0.103      0.125       0.129        0.285       0.197      0.0688\n# ℹ 11 more variables: `1_higher_managerial` &lt;dbl&gt;, `2_lower_managerial` &lt;dbl&gt;,\n#   `3_intermediate_occupations` &lt;dbl&gt;, `4_employers_small_org` &lt;dbl&gt;,\n#   `5_lower_supervisory` &lt;dbl&gt;, `6_semi_routine` &lt;dbl&gt;, `7_routine` &lt;dbl&gt;,\n#   `8_unemployed` &lt;dbl&gt;, avg_dwn_speed &lt;dbl&gt;, avb_superfast &lt;dbl&gt;,\n#   no_decent_bband &lt;dbl&gt;\n\n\nWe also need to rescale the data so all input data are presented on a comparable scale: the average download speed data (i.e. avg_dwn_speed) is very different to the other data that, for instance, represent the percentage of the population by different age groups.\n\n\n\nR code\n\n# rescale\ncluster_data &lt;- scale(cluster_data)\n\n# inspect\nhead(cluster_data)\n\n\n      age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n[1,] -1.7579913 -2.8680309 -0.36823669   2.1768529   0.2561260  -0.6039803\n[2,]  1.9519376  1.6403191 -0.05766949   0.1671857  -1.5639740   0.6215590\n[3,]  1.3549320  0.5475394  0.02888699   0.5773908  -0.7424464  -0.4769094\n[4,] -0.1050147  0.5938501  0.40759191  -0.2402277  -0.2247116   0.3136111\n[5,]  1.9687596  1.5173858  0.05639359   0.4733916  -0.9785969  -0.9539525\n[6,]  2.6064366  1.8434062  0.22116720   0.2379171  -1.3168781  -0.3384050\n     1_higher_managerial 2_lower_managerial 3_intermediate_occupations\n[1,]           4.4185012          1.8391416                 -2.2291104\n[2,]          -0.8510113         -0.9272602                  0.1031344\n[3,]          -0.4499409         -0.3266302                  1.5123844\n[4,]          -0.6741298         -0.3469851                  1.8104058\n[5,]          -0.9705088         -1.0065195                  0.5809449\n[6,]          -0.8571773         -1.0674213                 -0.1494470\n     4_employers_small_org 5_lower_supervisory 6_semi_routine   7_routine\n[1,]           -1.02298662         -2.53130431    -2.49888739 -1.81145433\n[2,]            0.39745656         -0.67859270     1.05051667  0.08877827\n[3,]            0.28308676         -0.37987493    -0.05595897 -0.52802287\n[4,]            0.13591535          0.24587683    -0.15093353 -0.07008362\n[5,]            0.09942212          0.42532220     0.76747039  0.35318453\n[6,]           -0.18786941          0.05518013     0.56442659  0.57156979\n     8_unemployed avg_dwn_speed avb_superfast no_decent_bband\n[1,]   -0.5139854    -1.6561653    -5.2186970      -0.3797659\n[2,]    1.1777334     0.7915882     0.5093876      -0.2031415\n[3,]    0.7615830     0.6354463     0.5222597      -0.3797659\n[4,]    0.2898589     0.9477301     0.5480039      -0.2472976\n[5,]    0.8482656     0.4701196     0.2648177       0.2384195\n[6,]    1.5510655     0.6813704     0.4965155      -0.1589854\n\n\nNow our data are all on the same scale, we will start by creating an elbow plot. The elbow method is a visual aid that can help in determining the number of clusters in a data set. Remember: this is important because with a k-means clustering you need to specify the numbers of clusters a priori!\nThe elbow method can help as it plots the total explained variation (‘Within Sum of Squares’) in your data as a function of the number of cluster. The idea is that you pick the number of clusters at the ‘elbow’ of the curve as this is the point in which the additional variation that would be explained by an additional cluster is decreasing. Effectively this means you are actually running the k-means clustering multiple times before running the actual k-means clustering algorithm.\n\n\n\nR code\n\n# create empty list to store the within sum of square values\nwss_values &lt;- list()\n\n# execute a k-means clustering for k=1, k=2, ..., k=15\nfor (i in 1:15) {\n    wss_values[i] &lt;- sum(kmeans(cluster_data, centers = i, iter.max = 30)$withinss)\n}\n\n# inspect\nwss_values\n\n\n[[1]]\n[1] 144143\n\n[[2]]\n[1] 110934.1\n\n[[3]]\n[1] 100042.6\n\n[[4]]\n[1] 82701.63\n\n[[5]]\n[1] 73974.93\n\n [ reached getOption(\"max.print\") -- omitted 10 entries ]\n\n\n\n\n\nR code\n\n# vector to dataframe\nwss_values &lt;- as.data.frame(wss_values)\n\n# transpose\nwss_values &lt;- as.data.frame(t(wss_values))\n\n# add cluster numbers\nwss_values$cluster &lt;- seq.int(nrow(wss_values))\nnames(wss_values) &lt;- c(\"wss\", \"cluster\")\n\n# plot using ggplot2\nggplot(data = wss_values, aes(x = cluster, y = wss)) +\n  geom_point() +\n  geom_path() +\n  scale_x_continuous(breaks = seq(1, 15)) +\n  xlab(\"Number of clusters\") +\n  ylab(\"Within sum of squares\") +\n  theme_minimal()\n\n\n\n\n\nFigure 6: Within sum of squares by number of clusters\n\n\n\n\nBased on the elbow plot, we can now choose the number of clusters and it looks like 7 clusters would be a reasonable choice.\n\n\n\n\n\n\nThe interpretation of an elbow plot can be quite subjective and often multiple options would be justified: 6, 8, and perhaps 9 clusters also do not look unreasonable. You would need to try the different options and see what output you get to determine the ‘optimal’ solution. However, at very least, the elbow plot does give you an idea of what would potentially be an adequate number of clusters.\n\n\n\nNow we have decided on the number of clusters (i.e. 7 clusters), we can run our cluster analysis. We will be running this analysis 10 times because there is an element of randomness within the clustering, and we want to make sure we get the optimal clustering output.\n\n\n\nR code\n\n# create empty list to store the results of the clustering\nclusters &lt;- list()\n\n# create empty variable to store fit\nfit &lt;- NA\n\n# run the k-means 10 times\nfor (i in 1:10) {\n\n    # keep track of the runs\n    print(paste0(\"starting run: \", i))\n\n    # run the k-means clustering algorithm to extract 7 clusters\n    clust7 &lt;- kmeans(x = cluster_data, centers = 7, iter.max = 1e+06, nstart = 1)\n\n    # get the total within sum of squares for the run and\n    fit[i] &lt;- clust7$tot.withinss\n\n    # update the results of the clustering if the total within sum of squares\n    # for the run is lower than any of the runs that have been executed so far\n    if (fit[i] &lt; min(fit[1:(i - 1)])) {\n        clusters &lt;- clust7\n    }\n}\n\n\n[1] \"starting run: 1\"\n[1] \"starting run: 2\"\n[1] \"starting run: 3\"\n[1] \"starting run: 4\"\n[1] \"starting run: 5\"\n[1] \"starting run: 6\"\n[1] \"starting run: 7\"\n[1] \"starting run: 8\"\n[1] \"starting run: 9\"\n[1] \"starting run: 10\"\n\n# inspect\nclusters\n\nK-means clustering with 7 clusters of sizes 955, 2394, 1885, 759, 582, 187, 1718\n\nCluster means:\n   age0to4pc  age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n  1_higher_managerial 2_lower_managerial 3_intermediate_occupations\n  4_employers_small_org 5_lower_supervisory 6_semi_routine  7_routine\n  8_unemployed avg_dwn_speed avb_superfast no_decent_bband\n [ reached getOption(\"max.print\") -- omitted 7 rows ]\n\nClustering vector:\n[1] 4 1 2 2 1\n [ reached getOption(\"max.print\") -- omitted 8475 entries ]\n\nWithin cluster sum of squares by cluster:\n[1]  8146.662 14203.994 10426.288  7998.965  6828.975\n [ reached getOption(\"max.print\") -- omitted 2 entries ]\n (between_SS / total_SS =  56.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [ reached getOption(\"max.print\") -- omitted 4 entries ]\n\n\nWe now have to execute a bit of post-processing to extract some useful summary data for each cluster: the cluster size (size) and mean values for each cluster.\n\n\n\nR code\n\n# assign to new variable for clarity\nkfit &lt;- clusters\n\n# cluster sizes\nkfit_size &lt;- kfit$size\n\n# inspect\nkfit_size\n\n\n[1]  955 2394 1885  759  582\n [ reached getOption(\"max.print\") -- omitted 2 entries ]\n\n# mean values for each variable in each cluster\nkfit_mean &lt;- as_tibble(aggregate(cluster_data, by = list(kfit$cluster), FUN = mean))\nnames(kfit_mean)[1] &lt;- \"cluster\"\n\n# inspect\nkfit_mean\n\n# A tibble: 7 × 18\n  cluster age0to4pc age5to14pc age16to24pc age25to44pc age45to64pc age75pluspc\n    &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1       1     1.68      1.22        0.401       0.719      -1.25      -1.03   \n2       2    -0.160    -0.0305     -0.201      -0.138       0.290      0.134  \n3       3     0.180     0.0509      0.0154     -0.0634     -0.0315    -0.00715\n4       4     0.351    -0.886       0.201       2.04       -1.22      -0.948  \n5       5    -0.891    -0.171      -0.520      -1.00        1.21       0.594  \n6       6    -1.52     -2.68        5.40        0.125      -2.62      -1.28   \n7       7    -0.596     0.0470     -0.460      -0.713       0.740      0.753  \n# ℹ 11 more variables: `1_higher_managerial` &lt;dbl&gt;, `2_lower_managerial` &lt;dbl&gt;,\n#   `3_intermediate_occupations` &lt;dbl&gt;, `4_employers_small_org` &lt;dbl&gt;,\n#   `5_lower_supervisory` &lt;dbl&gt;, `6_semi_routine` &lt;dbl&gt;, `7_routine` &lt;dbl&gt;,\n#   `8_unemployed` &lt;dbl&gt;, avg_dwn_speed &lt;dbl&gt;, avb_superfast &lt;dbl&gt;,\n#   no_decent_bband &lt;dbl&gt;\n\n\n\n\n\nR code\n\n# transform shape to tidy format\nkfit_mean_long &lt;- pivot_longer(kfit_mean, cols = (-cluster))\n\n# plot using ggplot2\nggplot(kfit_mean_long, aes(x = cluster, y = value, fill = name)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_x_continuous(breaks = seq(1, 7, by = 1)) +\n  xlab(\"Cluster\") +\n  ylab(\"Mean value\") +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\nFigure 7: Mean variable values by cluster\n\n\n\n\n\n\n\n\n\n\nYour values may be slightly different due to the random choice of initial cluster centres.\n\n\n\nLooking at the table with the mean values for each cluster and the barplot, we can see that variables contribute differently to the different clusters. The graph is a little busy, so you might want to look at the cluster groups or variables individually to get a better picture of each cluster.\n\n\n\nR code\n\n# plot using ggplot2\nggplot(kfit_mean_long[kfit_mean_long$cluster == 1, ], aes(x = cluster, y = value, fill = name)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  xlab(\"Cluster\") +\n  ylab(\"Mean value\") +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\nFigure 8: Mean variable values for cluster 1\n\n\n\n\nWe can also show the results of our geodemographic classification on a map.\n\n\n\nR code\n\n# read shape\nmsoa &lt;- st_read(\"data/boundaries/gb_msoa11_sim.shp\")\n\n\nReading layer `gb_msoa11_sim' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114Q/data/boundaries/gb_msoa11_sim.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8480 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5513 ymin: 5342.7 xmax: 655604.7 ymax: 1220302\nGeodetic CRS:  WGS 84\n\n# set projection\nst_crs(msoa) = 27700\n\n# simplify for speedier plotting\nmsoa &lt;- st_simplify(msoa, dTolerance = 100)\n\n# join\ncluster_data &lt;- cbind(iuc_input, kfit$cluster)\nmsoa &lt;- left_join(msoa, cluster_data, by = c(geo_code = \"msoa11cd\"))\n\n# plot\ntm_shape(msoa) + tm_fill(col = \"kfit$cluster\") + tm_layout(legend.outside = TRUE)\n\n\n\n\nFigure 9: Spatial pattern of our geodemographic classification\n\n\n\n\n\n\n\n\n\n\nYour map might look different as the cluster numbers are not consistently assigned and therefore colours can be map onto the cluster numbers differently.\n\n\n\n\n\n\n\nThe creation of a geodemographic classification is an iterative process of trial and error that involves the addition and removal of variables as well as experimentation with different numbers of clusters. It also might be, for instances, that some clusters are very focused on one group of data (e.g. age) and it could be an idea to group some ages together. If you want to make changes to the clustering solution, you can simply re-run the analysis with a different set of variables or with a different number of clusters by updating the code. However, it would be even simpler if you could automate some of the process.\nTry to create a R function that:\n\ntakes at least the following three arguments: a data frame that contains your input data, the number of clusters that you want, and a list of input variables;\nexecutes a k-means clustering on the input variables and the specified number of clusters; and,\nproduces a csv file that contains the table of means of the solution.\n\n\n\n\n\n\n\n\nYour function could look something like: run-kmeans(dataframe,number_of_clusters,input_variables)\nThe list of input variables does not have to be a list of names, but can also be a list containing the index values of the columns.\nHave a look at Hadley Wickhams explanation of functions in R."
  },
  {
    "objectID": "01-geodemographics.html#byl-geo",
    "href": "01-geodemographics.html#byl-geo",
    "title": "Geodemographics",
    "section": "",
    "text": "Having finished this tutorial, you should now understand the basics of a geodemographic classification. In addition, you should have written a simple function. Although you have now reached the end of this week’s content, you could try and improve your function. Consider:\n\nIncluding maps or graphs in the code that get automatically saved.\nEnsuring that the csv outcome does not get overwritten every time you run you function.\nIncluding optional arguments in your function with default values if certain values are not specified."
  },
  {
    "objectID": "02-network.html",
    "href": "02-network.html",
    "title": "Transport Network Analysis",
    "section": "",
    "text": "This week we will cover a different type of data: network data. We will take a look at how we can use network data to measure accessibility using the dodgr R library. We will calculate the network distances between combinations of locations (i.e. a set of origins and a set of destinations). These distances can then, for instance, be used to calculate the number of a resource (e.g. fast-food outlets) within a certain distance of a Point of Interest (e.g. a school or population-weighted centroid).\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nGeurs, K., Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\nNeutens, T. Schwanen, T. and Witlox, F. 2011. The prism of everyday life: Towards a new research agenda for time geography. Transport Reviews 31(1): 25-47. [Link]\n\n\n\n\n\nSchwanen, T. and De Jong, T. 2008. Exploring the juggling of responsibilities with space-time accessibility analysis. Urban Geography 29(6): 556-580. [Link]\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]\n\n\n\n\n\nThe term network analysis covers a wide range of analysis techniques ranging from complex network analysis to social network analysis, and from link analysis to transport network analysis. What the techniques have in common is that they are based on the concept of a network. A network or network graph is constituted by a collection of vertices that are connected to one another by edges. Note, vertices may also be called nodes or points, whilst edges may be called links or lines. Within social network analysis, you may find the terms actors (the vertices) and ties or relations (the edges) also used.\n\n\n\n\n\nFigure 1: Visualising networks with vertices and edges.\n\n\n\n\n\n\nFor this week’s practical, we will be using Portsmouth in the UK as our area of interest for our analysis. One prominent topic within the city is the issue of public health and childhood obesity. According to figures released in March 2020 by Public Health England, more than one in three school pupils are overweight or obese by the time they finish primary school within the city; this is much higher than the national average of one in four. One potential contributor to the health crisis is the ease and availability of fast-food outlets in the city. In the following, we will measure the accessibility of fast-food outlets within specific walking distances of all school in Portsmouth starting at 400m, then 800m and finally a 1km walking distance. We will then aggregate these results to Lower Super Output Areas (LSOA) and overlay these results with some socio-economic variables.\nTo execute this analysis, we will need to first calculate the distances between our schools and fast-food outlets. This involves calculating the shortest distance a child would walk between a school and a fast-food outlet, using roads or streets. We will use the dodgr R package to conduct this transport network analysis.\n\n\n\n\n\n\nAll calculations within the dodgr library currently need to be run in WGS84/4236. This is why we will not transform the CRS of our data in this practical.\n\n\n\n\n\n\nAs usual, we will start by loading any libraries we will require. Install any libraries that you might not have installed before.\n\n\n\nR code\n\n# libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\nTo create our network and Origin-Destination dataset, we will need data on schools, fast-food outlets, and a streetnetwork. Today we will be using OpenStreetMap for this. If you have never come across OpenStreetMap (OSM) before, it is a free editable map of the world.\n\n\n\n\n\n\nOpenStreetMap’s spatial coverage is still unequal across the world as well as that, as you will find if you use the data, the accuracy and quality of the data can often be quite questionable or simply missing attribute details that we would like to have, e.g. types of roads and their speed limits.\n\n\n\nWhilst there are various approaches to downloading data from OpenStreetMap, we will use the osmdata library to directly extract our required OpenStreetMap (OSM) data into a variable. The osmdata library grants access within R to the Overpass API that allows us to run queries on OSM data and then import the data as spatial objects. These queries are at the heart of these data downloads.\nWe will go ahead and start with downloading and extracting our road network data. To OSM data using the osmdata library, we can use the add_osm_feature() function. To use the function, we need to provided it with either a bounding box of our area of interest (AOI) or a set of points, from which the function will create its own bounding box. You can find out more about this and details on how to construct your own queries in the data vignette.\n\n\n\n\n\n\nA bounding box, or bbox, is an area defined by two longitudes and two latitudes. Essentially, it is a rectangular georeferenced polygon that you can use to demarcate an area. You can either define bounding box coordinates yourself or extract values from an existing shapefile or GeoPackage.\n\n\n\nTo use the library (and API), we need to know how to write and run a query, which requires identifying the key and value that we need within our query to select the correct data. Essentially every map element (whether a point, line or polygon) in OSM is tagged with different attribute data. These keys and values are used in our queries to extract only map elements of that feature type - to find out how a feature is tagged in OSM is simply a case of reading through the OSM documentation and becoming familiar with their keys and values.\nTo download our road network dataset, we first define a variable to store our bounding box coordinates, p_bbox(). We then use this within our OSM query to extract specific types of road segments within that bounding box - the results of our query are then stored in an osmdata object. We will select all OSM features with the highway tag that are likely to be used by pedestrians (e.g. not motorways).\n\n\n\nR code\n\n# define our bbox coordinates for Portsmouth\np_bbox &lt;- c(-1.113197, 50.775781, -1.026508, 50.859941)\n\n# pass bounding box coordinates into the OverPassQuery (opq) function only\n# download features that are not classified as motorway\nosmdata &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some instances the OSM query will return an error, especially when several people from the same location are executing the exact same query. If this happens, you can just read through the instructions and download a prepared copy of the data that contains all required OSM Portsmouth data instead: [Link].\nYou can load these downloaded data as follows into R:\n\n\n\nR code\n\nload(\"../path/to/file/ports_ff.RData\")\nload(\"../path/to/file/ports_roads_edges.RData\")\nload(\"../path/to/file/ports_schools.RData\")\n\n\nAfter loading your data, you can continue with the analysis in the Measuring Accessiblity section below, starting with the creation of a network graph with the ‘foot weighting’ profile.\n\n\n\nThe osmdata object contains the bounding box of your query, a time-stamp of the query, and then the spatial data as osm_points, osm_lines, osm_multilines and osm_polgyons (which are listed with their respective fields also detailed). Some of the spatial features maybe empty, depending on what you asked your query to return. Our next step therefore is to extract our spatial data from our osmdata object to create our road network data set. This is in fact incredibly easy, using the traditional $ R approach to access these spatial features from our object.\nDeciding what to extract is probably the more complicated aspect of this - mainly as you need to understand how to represent your road network, and this will usually be determined by the library/functions you will be using it within. Today, we want to extract the edges of the network, i.e. the lines that represent the roads, as well as the nodes of the network, i.e. the points that represent the locations at which the roads start, end, or intersect. For our points, we will only keep the osm_id data field, just in case we need to refer to this later. For our lines, we will keep a little more information that we might want to use within our transport network analysis, including the type of road, the maximum speed, and whether the road is one-way or not.\n\n\n\nR code\n\n# extract the `p`oints, with their osm_id.\nports_roads_nodes &lt;- osmdata$osm_points[, \"osm_id\"]\n\n# extract the lines, with their osm_id, name, type of highway, max speed and\n# oneway attributes\nports_roads_edges &lt;- osmdata$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n\nTo check our data set, we can quickly plot the edges of our road network using the plot() function:\n\n\n\nR code\n\n# inspect\nplot(ports_roads_edges, max.plot = 1)\n\n\n\n\n\nFigure 2: OSM road network\n\n\n\n\nBecause we are focusing on walking, we will overwrite the oneway variable by suggesting that none of the road segments are restricted to one-way traffic which may affect our analysis as well as the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nports_roads_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics.\n\n\n\nBefore we can construct our full network graph for the purpose of accessibility analysis, we need to also provide our Origin and Destination points, i.e. the data points we wish to calculate the distances between. According to the dodgr documentation, these points need to be in either a vector or matrix format, containing the two coordinates for each point for the origins and for the destinations.\nAs for our Portsmouth scenario we are interested in calculating the shortest distances between schools and fast-food outlets, we need to try and download these datasets from OpenStreetMap as well. Following a similar structure to our query above, we will use our knowledge of OpenStreetMap keys and values to extract the points of Origins (schools) and Destinations (fast-food outlets) we are interested in:\n\n\n\nR code\n\n# download schools\nschools &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"school\") |&gt;\n    osmdata_sf()\n\n# download fast-food outlets\nff_outlets &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"fast_food\") |&gt;\n    osmdata_sf()\n\n\nWe also need to then extract the relevant data from the osmdata object:\n\n\n\nR code\n\n# extract school points\nports_schools &lt;- schools$osm_points[, c(\"osm_id\", \"name\")]\n\n# extract fast-food outlet points\nports_ff &lt;- ff_outlets$osm_points[, c(\"osm_id\", \"name\")]\n\n\nWe now have our road network data and our Origin-Destination (OD) points in place and we can now move to construct our network graph and run our transport network analysis.\n\n\n\n\n\n\nIn this analysis, we are highly reliant on the use of OpenStreetMap to provide data for both our Origins and Destinations. Whilst in the UK OSM provides substantial coverage, its quality is not always guaranteed. As a result, to improve on our current methodology in future analysis, we should investigate into a more official school data set or at least validate the number of schools against City Council records. The same applies to our fast-food outlets.\n\n\n\nWith any network analysis, the main data structure is a graph, constructed by our nodes and edges. To create a graph for use within dodgr, we pass our ports_roads_edges() into the weight_streetnet() function. The dodgr library also contains weighting profiles, that you can customise, for use within your network analysis. These weighting profiles contain weights based on the type of road, determined by the type of transportation the profile aims to model. Here we will use the weighting profile foot, as we are looking to model walking accessibility.\n\n\n\nR code\n\n# create network graph with the foot weighting profile\ngraph &lt;- weight_streetnet(ports_roads_edges, wt_profile = \"foot\")\n\n\nOnce we have our graph, we can then use this to calculate our network distances between our OD points. One thing to keep in mind is that potentially not all individual components in the network that we extracted are connected, for instance, because the bounding box cut off the access road of a cul-de-sac. To make sure that our entire extracted network is connected, we now extract the largest connected component of the graph. You can use table(graph$component) to examine the sizes of all individual subgraphs. You will notice that most subgraphs consist of a very small number of edges.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always denoting the largest component. Always inspect the resulting subgraph to make sure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\ngraph_connected &lt;- graph[graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(graph_connected)\n\n\n[1] 60700\n\n# inspect\nplot(dodgr_to_sf(graph_connected), max.plot = 1)\n\n\n\n\nFigure 3: Largest graph component\n\n\n\n\n\n\n\n\n\n\nOpenStreetMap is a living dataset, meaning that changes are made on a continuous basis; as such it may very well possible that the number of remaining road segments as shown above may be slighlty different when you run this analysis.\n\n\n\nNow we have our connected subgraph, will can use the dodgr_distances() function to calculate the network distances between every possible Origin and Destination. In the dodgr_distances() function, we first pass our graph, then our Origin points (schools), in the from argument, and then our Destination points (fast-food outlets), in the to argument. One thing to note is our addition of the st_coordinates() function as we pass our two point data sets within the from and to functions as we need to supplement our Origins and Destinations in a matrix format. For all Origins and Destinations, dodgr_distances() will map the points to the closest network points, and return corresponding shortest-path distances.\n\n\n\nR code\n\n# create a distance matrix between schools and fast-food stores\nsch_to_ff_calc &lt;- dodgr_distances(graph_connected, from = st_coordinates(ports_schools),\n    to = st_coordinates(ports_ff), shortest = TRUE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance-matrix that contains the network distances between all Origins (i.e. schools) and all Destinations (i.e. fast-food outlets). Let’s inspect the first row of our output. Do you understand what the values mean?\n\n\n\nR code\n\n# inspect\nhead(sch_to_ff_calc, n = 1)\n\n\n         3708702676 583409150 110151723 4179720607 112032935 1684258957\n35299419   4000.016  2090.485  6551.723    9067.55  10614.91   2231.919\n         35510611 6806456949 1319464203 1319464086 2537832173 1319464203\n35299419 11570.17   2292.263   1676.914   1680.151   1697.525   1676.914\n         583409150 583409150 583409150 3708702676 6806456947 3708702676\n35299419  2090.485  2090.485  2090.485   4000.016   2324.454   4000.016\n         3708702676 4741221735 3080970373 6486730562 2526286989 360754572\n35299419   4000.016    3338.84   1700.179   581.0582   7262.143  3370.976\n         1684760757 111811784 4547890993   596188 360666535 153334012 35309497\n35299419   1359.146  3321.384   340.8823 1102.226  10508.09  2644.278 2720.491\n         4559843487 533710034 1584811969 35309619 35309619 1584608863\n35299419   3260.791  2330.386   2580.465 2920.565 2920.565   736.0556\n         2530707658   210200 8501630407   163535 33033068 1517208796 1592759310\n35299419   3399.756 844.6083   6005.212 3394.408  8882.48   9083.916   8949.552\n         33024082 1322971868 1584776930 1787982426 128227681 3119584321\n35299419 9193.238   1629.029   3277.902   2400.918  6946.724   8688.361\n         33032892 112015327 12036553018 12036553018 360951843 360951843\n35299419 9114.322  9041.214    6503.173    6503.173  6470.933  6470.933\n         1684217275 1684048292 8788727818 1446611129 1446611129 1765156609\n35299419   804.5849   763.0456   2526.807   1010.408   1010.408   1967.697\n           688134 1584811969 117492188 1765137127 1496776935 1634771122\n35299419 1418.767   2580.465  1245.823    2157.02   688.8672   2162.922\n           691638   672367 12033597986 28836634 1765137126   210200 851157783\n35299419 2205.285 2119.323    11968.16 4603.517   2738.124 844.6083  615.1102\n         3357036324 8788727825 6170004942 117484085 35510611 35510611 35510611\n35299419   788.7212   2586.476   2667.125  728.4882 11570.17 11570.17 11570.17\n         35510611 35510611 35510611 35510611 35510611 35510611 35510611\n35299419 11570.17 11570.17 11570.17 11570.17 11570.17 11570.17 11570.17\n         35510611 35510611 35510611 2469323680 11549983260 11549983260\n35299419 11570.17 11570.17 11570.17    11051.2    6380.673    6380.673\n         11549983260 9563701017 3708702676 3708702676 1747135467 1682386860\n35299419    6380.673   6214.692   4000.016   4000.016   4234.058   5702.077\n         128349051 6486730562   691582 41466838 4533088712 4533088770\n35299419  3841.384   581.0582 597.3575 2654.314   4884.169   5081.596\n         5589038074 850508342 4639702744 5589038074 3227275829 1684055602\n35299419   4823.045  375.2084   926.0127   4823.045   3808.049    753.752\n           474557 1584608915 1517984612 11513738570 1684226066 27679037\n35299419 556.5173    713.536   622.6949    619.2467    1225.66 3452.874\n         1381614134 5337216850 11549983251 11549983257 11549983257 11549983252\n35299419   3363.827   5226.252    6439.123      6411.8      6411.8    6415.946\n         11549983252 11549983252 11549983252 11549983251 11549983251\n35299419    6415.946    6415.946    6415.946    6439.123    6439.123\n         11549983252 11549983247 11549983251 11549983251 11549983251\n35299419    6415.946    6453.555    6439.123    6439.123    6439.123\n         11549983251 12034855970 1517209100 1517209100 9445278102 4179720618\n35299419    6439.123    12081.21   9239.889   9239.889   5296.254    9050.93\n         4179720618 360692085 618271588 360692085 4179720618 12033597756\n35299419    9050.93  9002.504  8275.415  9002.504    9050.93    11708.13\n         12033597753 12033597764 12033597762 9307660916 11364252738 1684259012\n35299419    11696.37    11754.26    11722.44   9259.759    3349.922   2300.121\n         4361632708 1314915645 7006050181   474383   474383   474383 1448664912\n35299419   815.5434   3219.572   5485.599 905.8238 905.8238 905.8238   4950.036\n         26658915 118724163 1804412114 107228955 10713423   547019 3754347320\n35299419 4968.988  858.9023   3216.095  3159.418 11347.78 4919.699   1813.109\n         33033074 3080970374 20464883 1517208858 107145004 7028566698\n35299419 8871.589   1666.407 9310.528   9370.483  4462.093   3177.182\n         1314915645 36866188 1682829408 5433229750 1517208858   194076\n35299419   3219.572 3118.604   5756.392   9250.383   9370.483 5462.571\n         130069978 360689836 108044084 108044703 111996556 1684631749 107912783\n35299419  4046.518  8931.139  6193.605  6126.632   8616.91   6647.454  6049.243\n         112032936   210208 1240746711 1917246883   596120 11659717644\n35299419  10483.53 1505.914   4995.748   11232.78 1574.826    5218.511\n         942789065 11527934107 1517209249   402702 26658915 361463301 131956097\n35299419  1804.091    3084.572   8813.936 4512.959 4968.988  3803.913  12902.97\n         11737454087 1381614134 1381614134   474557 3357036324 33032729\n35299419    5250.123   3363.827   3363.827 556.5173   788.7212   9824.4\n         4533088711 5433229750 5433229750 5433229750 4787197864 4787197864\n35299419   4866.125   9250.383   9250.383   9250.383   9232.061   9232.061\n         4787197864 4787197864 5433229750 5433229750 5837229773 6992272607\n35299419   9232.061   9232.061   9250.383   9250.383   5774.382   5383.464\n          402703 361463301 1684581956 1684582104 35298117 849545628 850213112\n35299419 4492.53  3803.913   4661.927   4913.756 1543.604  1510.665  1559.537\n         4533088743 1917247132 5478469295 5478469294   470102 1584776930\n35299419   5081.936   11390.34   4719.045   4723.757 4271.398   3277.902\n         1682809610 11659717644 4533088770 4533088770 4533088770 4533088770\n35299419   5819.778    5218.511   5081.596   5081.596   5081.596   5081.596\n         4533088770 4533088770 4533088770   320774 8501630403 107887646\n35299419   5081.596   5081.596   5081.596 4963.868   6045.425   5140.89\n           402702 1681595494 11549983252 8489626386 1917246879 1917246684\n35299419 4512.959   5601.532    6415.946   4625.997   11303.07   11327.69\n         5589038074 5589038074 5589038074 5589038074 4533088711 4533088714\n35299419   4823.045   4823.045   4823.045   4823.045   4866.125   4895.861\n         4533088711 4533088714 4533088714 4533088713 130240118 110151723\n35299419   4866.125   4895.861   4895.861   4890.115  11885.28  6551.723\n         110151723 110151723 110151723 110151723 110151723 29368594 26658915\n35299419  6551.723  6551.723  6551.723  6551.723  6551.723 4582.555 4968.988\n         7305624459 106007661 8987883618 5859802411   518610   518610   518611\n35299419   11517.33  4548.575   4136.321   1129.672 1302.289 1302.289 1320.727\n         800803720   290950 4081504238 4081504206 4533088742 35510611\n35299419  3344.675 4464.824   1332.334   1122.165   5061.751 11570.17\n         12033597756 6732089007 12036553018 7006091789 4559843598 5049080642\n35299419    11708.13    3002.79    6503.173   5562.273   3135.612   3166.975\n         1917246684 35309619 1740407820 158373125 7006142009 7538876197\n35299419   11327.69 2920.565    1677.06  2312.611   5620.269   5518.542\n         1787929945 7006142009 7006142009 4533088693 851157206 1592759339\n35299419   5520.407   5620.269   5620.269   4733.129  1128.534   8944.679\n          191611 107545686 8788625297 8788625298 8788625298 4081504206\n35299419 2060.98  7498.559   1114.958   1092.273   1092.273   1122.165\n         8788625298 4936929522   691638 2113130392 8788728291 4741418602\n35299419   1092.273    4661.76 2205.285   3361.825   2738.453   3026.335\n         9465974729 4559843597 4559843597 360754572 360754572 1947998799\n35299419   3260.599   3275.482   3275.482  3370.976  3370.976   3400.026\n         107228953 4559843597 360465477   691582 1584777018 7111358956\n35299419  3139.566   3275.482   2487.06 597.3575    3294.96   7368.158\n         6801562238 6801562238 7111358956\n35299419    7329.26    7329.26   7368.158\n\n\nOur output shows the calculations for the first school - and the distances between the school and every fast-food outlet. Because we manually overwrote the values for all one-way streets as well as that we extracted the larges connected graph only, we currently should not have any NA values.\n\n\n\n\n\n\nThe dodgr vignette notes that a distance matrix obtained from running dodgr_distances on graph_connected should generally contain no NA values, although some points may still be effectively unreachable due to one-way connections (or streets). Thus, routing on the largest connected component of a directed graph ought to be expected to yield the minimal number of NA values, which may sometimes be more than zero. Note further that spatial routing points (expressed as from and/or to arguments) will in this case be mapped to the nearest vertices of graph_connected, rather than the potentially closer nearest points of the full graph.\n\n\n\nThe next step of processing all depends on what you are trying to assess. Today we want to understand which schools have a closer proximity to fast-food outlets and which do not, quantified by how many outlets are within walking distance. We will therefore look to count how many outlets are with walking distance from each school and store this as a new column within our ports_school data frame.\n\n\n\nR code\n\n# fastfood outlets within 400m\nports_schools$ff_within_400m &lt;- rowSums(sch_to_ff_calc &lt;= 400)\n\n# fastfood outlets within 800m\nports_schools$ff_within_800m &lt;- rowSums(sch_to_ff_calc &lt;= 800)\n\n# fastfood outlets within 1000m\nports_schools$ff_within_1km &lt;- rowSums(sch_to_ff_calc &lt;= 1000)\n\n\nYou can inspect the ports_schools object to see the results of this analysis.\n\n\n\nNow you have calculated the number of fast-food outlets within specific distances from every school in Portsmouth and should get the idea behind a basic accessibility analysis, your task is to estimate the accessibility of fast-food outlets at the LSOA scale and compare this to the 2019 Index of Multiple Deprivation.\n\n\n\n\n\n\nThis skills and steps required for this analysis are not just based on this week’s practical, but you will have to combine all your knowledge of coding and spatial analysis you have gained over the past weeks.\n\n\n\nOne way of doing this, is by taking some of the following steps:\n\nDownload the 2011 LSOA boundaries and extract only those that relate to Portsmouth.\nDownload the 2019 Index of Multiple Deprivation scores.\nDecide on an accessibility measure, such as:\n\nThe average number of fast-food restaurants within x meters of a school within each LSOA.\nThe average distance a fast-food restaurant is from a school within each LSOA.\nThe (average) shortest distance a fast-food restaurant is from a school within each LSOA.\nThe minimum shortest distance a fast-food outlet is from a school within each LSOA.\n\nAggregate accessibility scores to the LSOA level.\nJoin the 2019 Index of Multiple Deprivation data to your LSOA dataset.\nFor each IMD decile, calculate the average for your chosen aggregate measure and produce a table.\n\nUsing your approach what do you think: are fast-food restaurants, on average, more accessible for students at schools that are located within LSOAs with a lower IMD decile (more deprived) when compared to students at schools that are located within LSOAs with a higher IMD decile (less deprived)?\n\n\n\n\nWe have now conducted some basic accessibility analysis, however, there is some additional fundamental challenges to consider in the context of transport network and accessibility analysis:\n\nHow do the different weight profiles of the dodgr package work? How would one go about creating your own weight profile? How would using a different weight profiles affect the results of your analysis?\nWhy do we have unconnected segments in the extracted transport network? How would you inspect these unconnected segments? Would they need to be connected? If so, how would one do this?\nWhy you think all Origins and Destinations are mapped onto the closest network points? Is this always the best option? What alternative methods could you think of and how would you implement these?\n\n\n\n\n\n\n\nIf you want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R.\n\n\n\n\n\n\nHaving finished this tutorial on transport network analysis and, hopefully, having been able to independently conduct some further area-profiling using IMD deciles, you have now reached the end of this week’s content."
  },
  {
    "objectID": "02-network.html#lecture-w08",
    "href": "02-network.html#lecture-w08",
    "title": "Transport Network Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "02-network.html#reading-w08",
    "href": "02-network.html#reading-w08",
    "title": "Transport Network Analysis",
    "section": "",
    "text": "Geurs, K., Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\nNeutens, T. Schwanen, T. and Witlox, F. 2011. The prism of everyday life: Towards a new research agenda for time geography. Transport Reviews 31(1): 25-47. [Link]\n\n\n\n\n\nSchwanen, T. and De Jong, T. 2008. Exploring the juggling of responsibilities with space-time accessibility analysis. Urban Geography 29(6): 556-580. [Link]\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]"
  },
  {
    "objectID": "02-network.html#transport-network-analysis-1",
    "href": "02-network.html#transport-network-analysis-1",
    "title": "Transport Network Analysis",
    "section": "",
    "text": "The term network analysis covers a wide range of analysis techniques ranging from complex network analysis to social network analysis, and from link analysis to transport network analysis. What the techniques have in common is that they are based on the concept of a network. A network or network graph is constituted by a collection of vertices that are connected to one another by edges. Note, vertices may also be called nodes or points, whilst edges may be called links or lines. Within social network analysis, you may find the terms actors (the vertices) and ties or relations (the edges) also used.\n\n\n\n\n\nFigure 1: Visualising networks with vertices and edges.\n\n\n\n\n\n\nFor this week’s practical, we will be using Portsmouth in the UK as our area of interest for our analysis. One prominent topic within the city is the issue of public health and childhood obesity. According to figures released in March 2020 by Public Health England, more than one in three school pupils are overweight or obese by the time they finish primary school within the city; this is much higher than the national average of one in four. One potential contributor to the health crisis is the ease and availability of fast-food outlets in the city. In the following, we will measure the accessibility of fast-food outlets within specific walking distances of all school in Portsmouth starting at 400m, then 800m and finally a 1km walking distance. We will then aggregate these results to Lower Super Output Areas (LSOA) and overlay these results with some socio-economic variables.\nTo execute this analysis, we will need to first calculate the distances between our schools and fast-food outlets. This involves calculating the shortest distance a child would walk between a school and a fast-food outlet, using roads or streets. We will use the dodgr R package to conduct this transport network analysis.\n\n\n\n\n\n\nAll calculations within the dodgr library currently need to be run in WGS84/4236. This is why we will not transform the CRS of our data in this practical.\n\n\n\n\n\n\nAs usual, we will start by loading any libraries we will require. Install any libraries that you might not have installed before.\n\n\n\nR code\n\n# libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\nTo create our network and Origin-Destination dataset, we will need data on schools, fast-food outlets, and a streetnetwork. Today we will be using OpenStreetMap for this. If you have never come across OpenStreetMap (OSM) before, it is a free editable map of the world.\n\n\n\n\n\n\nOpenStreetMap’s spatial coverage is still unequal across the world as well as that, as you will find if you use the data, the accuracy and quality of the data can often be quite questionable or simply missing attribute details that we would like to have, e.g. types of roads and their speed limits.\n\n\n\nWhilst there are various approaches to downloading data from OpenStreetMap, we will use the osmdata library to directly extract our required OpenStreetMap (OSM) data into a variable. The osmdata library grants access within R to the Overpass API that allows us to run queries on OSM data and then import the data as spatial objects. These queries are at the heart of these data downloads.\nWe will go ahead and start with downloading and extracting our road network data. To OSM data using the osmdata library, we can use the add_osm_feature() function. To use the function, we need to provided it with either a bounding box of our area of interest (AOI) or a set of points, from which the function will create its own bounding box. You can find out more about this and details on how to construct your own queries in the data vignette.\n\n\n\n\n\n\nA bounding box, or bbox, is an area defined by two longitudes and two latitudes. Essentially, it is a rectangular georeferenced polygon that you can use to demarcate an area. You can either define bounding box coordinates yourself or extract values from an existing shapefile or GeoPackage.\n\n\n\nTo use the library (and API), we need to know how to write and run a query, which requires identifying the key and value that we need within our query to select the correct data. Essentially every map element (whether a point, line or polygon) in OSM is tagged with different attribute data. These keys and values are used in our queries to extract only map elements of that feature type - to find out how a feature is tagged in OSM is simply a case of reading through the OSM documentation and becoming familiar with their keys and values.\nTo download our road network dataset, we first define a variable to store our bounding box coordinates, p_bbox(). We then use this within our OSM query to extract specific types of road segments within that bounding box - the results of our query are then stored in an osmdata object. We will select all OSM features with the highway tag that are likely to be used by pedestrians (e.g. not motorways).\n\n\n\nR code\n\n# define our bbox coordinates for Portsmouth\np_bbox &lt;- c(-1.113197, 50.775781, -1.026508, 50.859941)\n\n# pass bounding box coordinates into the OverPassQuery (opq) function only\n# download features that are not classified as motorway\nosmdata &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some instances the OSM query will return an error, especially when several people from the same location are executing the exact same query. If this happens, you can just read through the instructions and download a prepared copy of the data that contains all required OSM Portsmouth data instead: [Link].\nYou can load these downloaded data as follows into R:\n\n\n\nR code\n\nload(\"../path/to/file/ports_ff.RData\")\nload(\"../path/to/file/ports_roads_edges.RData\")\nload(\"../path/to/file/ports_schools.RData\")\n\n\nAfter loading your data, you can continue with the analysis in the Measuring Accessiblity section below, starting with the creation of a network graph with the ‘foot weighting’ profile.\n\n\n\nThe osmdata object contains the bounding box of your query, a time-stamp of the query, and then the spatial data as osm_points, osm_lines, osm_multilines and osm_polgyons (which are listed with their respective fields also detailed). Some of the spatial features maybe empty, depending on what you asked your query to return. Our next step therefore is to extract our spatial data from our osmdata object to create our road network data set. This is in fact incredibly easy, using the traditional $ R approach to access these spatial features from our object.\nDeciding what to extract is probably the more complicated aspect of this - mainly as you need to understand how to represent your road network, and this will usually be determined by the library/functions you will be using it within. Today, we want to extract the edges of the network, i.e. the lines that represent the roads, as well as the nodes of the network, i.e. the points that represent the locations at which the roads start, end, or intersect. For our points, we will only keep the osm_id data field, just in case we need to refer to this later. For our lines, we will keep a little more information that we might want to use within our transport network analysis, including the type of road, the maximum speed, and whether the road is one-way or not.\n\n\n\nR code\n\n# extract the `p`oints, with their osm_id.\nports_roads_nodes &lt;- osmdata$osm_points[, \"osm_id\"]\n\n# extract the lines, with their osm_id, name, type of highway, max speed and\n# oneway attributes\nports_roads_edges &lt;- osmdata$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n\nTo check our data set, we can quickly plot the edges of our road network using the plot() function:\n\n\n\nR code\n\n# inspect\nplot(ports_roads_edges, max.plot = 1)\n\n\n\n\n\nFigure 2: OSM road network\n\n\n\n\nBecause we are focusing on walking, we will overwrite the oneway variable by suggesting that none of the road segments are restricted to one-way traffic which may affect our analysis as well as the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nports_roads_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics.\n\n\n\nBefore we can construct our full network graph for the purpose of accessibility analysis, we need to also provide our Origin and Destination points, i.e. the data points we wish to calculate the distances between. According to the dodgr documentation, these points need to be in either a vector or matrix format, containing the two coordinates for each point for the origins and for the destinations.\nAs for our Portsmouth scenario we are interested in calculating the shortest distances between schools and fast-food outlets, we need to try and download these datasets from OpenStreetMap as well. Following a similar structure to our query above, we will use our knowledge of OpenStreetMap keys and values to extract the points of Origins (schools) and Destinations (fast-food outlets) we are interested in:\n\n\n\nR code\n\n# download schools\nschools &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"school\") |&gt;\n    osmdata_sf()\n\n# download fast-food outlets\nff_outlets &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"fast_food\") |&gt;\n    osmdata_sf()\n\n\nWe also need to then extract the relevant data from the osmdata object:\n\n\n\nR code\n\n# extract school points\nports_schools &lt;- schools$osm_points[, c(\"osm_id\", \"name\")]\n\n# extract fast-food outlet points\nports_ff &lt;- ff_outlets$osm_points[, c(\"osm_id\", \"name\")]\n\n\nWe now have our road network data and our Origin-Destination (OD) points in place and we can now move to construct our network graph and run our transport network analysis.\n\n\n\n\n\n\nIn this analysis, we are highly reliant on the use of OpenStreetMap to provide data for both our Origins and Destinations. Whilst in the UK OSM provides substantial coverage, its quality is not always guaranteed. As a result, to improve on our current methodology in future analysis, we should investigate into a more official school data set or at least validate the number of schools against City Council records. The same applies to our fast-food outlets.\n\n\n\nWith any network analysis, the main data structure is a graph, constructed by our nodes and edges. To create a graph for use within dodgr, we pass our ports_roads_edges() into the weight_streetnet() function. The dodgr library also contains weighting profiles, that you can customise, for use within your network analysis. These weighting profiles contain weights based on the type of road, determined by the type of transportation the profile aims to model. Here we will use the weighting profile foot, as we are looking to model walking accessibility.\n\n\n\nR code\n\n# create network graph with the foot weighting profile\ngraph &lt;- weight_streetnet(ports_roads_edges, wt_profile = \"foot\")\n\n\nOnce we have our graph, we can then use this to calculate our network distances between our OD points. One thing to keep in mind is that potentially not all individual components in the network that we extracted are connected, for instance, because the bounding box cut off the access road of a cul-de-sac. To make sure that our entire extracted network is connected, we now extract the largest connected component of the graph. You can use table(graph$component) to examine the sizes of all individual subgraphs. You will notice that most subgraphs consist of a very small number of edges.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always denoting the largest component. Always inspect the resulting subgraph to make sure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\ngraph_connected &lt;- graph[graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(graph_connected)\n\n\n[1] 60700\n\n# inspect\nplot(dodgr_to_sf(graph_connected), max.plot = 1)\n\n\n\n\nFigure 3: Largest graph component\n\n\n\n\n\n\n\n\n\n\nOpenStreetMap is a living dataset, meaning that changes are made on a continuous basis; as such it may very well possible that the number of remaining road segments as shown above may be slighlty different when you run this analysis.\n\n\n\nNow we have our connected subgraph, will can use the dodgr_distances() function to calculate the network distances between every possible Origin and Destination. In the dodgr_distances() function, we first pass our graph, then our Origin points (schools), in the from argument, and then our Destination points (fast-food outlets), in the to argument. One thing to note is our addition of the st_coordinates() function as we pass our two point data sets within the from and to functions as we need to supplement our Origins and Destinations in a matrix format. For all Origins and Destinations, dodgr_distances() will map the points to the closest network points, and return corresponding shortest-path distances.\n\n\n\nR code\n\n# create a distance matrix between schools and fast-food stores\nsch_to_ff_calc &lt;- dodgr_distances(graph_connected, from = st_coordinates(ports_schools),\n    to = st_coordinates(ports_ff), shortest = TRUE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance-matrix that contains the network distances between all Origins (i.e. schools) and all Destinations (i.e. fast-food outlets). Let’s inspect the first row of our output. Do you understand what the values mean?\n\n\n\nR code\n\n# inspect\nhead(sch_to_ff_calc, n = 1)\n\n\n         3708702676 583409150 110151723 4179720607 112032935 1684258957\n35299419   4000.016  2090.485  6551.723    9067.55  10614.91   2231.919\n         35510611 6806456949 1319464203 1319464086 2537832173 1319464203\n35299419 11570.17   2292.263   1676.914   1680.151   1697.525   1676.914\n         583409150 583409150 583409150 3708702676 6806456947 3708702676\n35299419  2090.485  2090.485  2090.485   4000.016   2324.454   4000.016\n         3708702676 4741221735 3080970373 6486730562 2526286989 360754572\n35299419   4000.016    3338.84   1700.179   581.0582   7262.143  3370.976\n         1684760757 111811784 4547890993   596188 360666535 153334012 35309497\n35299419   1359.146  3321.384   340.8823 1102.226  10508.09  2644.278 2720.491\n         4559843487 533710034 1584811969 35309619 35309619 1584608863\n35299419   3260.791  2330.386   2580.465 2920.565 2920.565   736.0556\n         2530707658   210200 8501630407   163535 33033068 1517208796 1592759310\n35299419   3399.756 844.6083   6005.212 3394.408  8882.48   9083.916   8949.552\n         33024082 1322971868 1584776930 1787982426 128227681 3119584321\n35299419 9193.238   1629.029   3277.902   2400.918  6946.724   8688.361\n         33032892 112015327 12036553018 12036553018 360951843 360951843\n35299419 9114.322  9041.214    6503.173    6503.173  6470.933  6470.933\n         1684217275 1684048292 8788727818 1446611129 1446611129 1765156609\n35299419   804.5849   763.0456   2526.807   1010.408   1010.408   1967.697\n           688134 1584811969 117492188 1765137127 1496776935 1634771122\n35299419 1418.767   2580.465  1245.823    2157.02   688.8672   2162.922\n           691638   672367 12033597986 28836634 1765137126   210200 851157783\n35299419 2205.285 2119.323    11968.16 4603.517   2738.124 844.6083  615.1102\n         3357036324 8788727825 6170004942 117484085 35510611 35510611 35510611\n35299419   788.7212   2586.476   2667.125  728.4882 11570.17 11570.17 11570.17\n         35510611 35510611 35510611 35510611 35510611 35510611 35510611\n35299419 11570.17 11570.17 11570.17 11570.17 11570.17 11570.17 11570.17\n         35510611 35510611 35510611 2469323680 11549983260 11549983260\n35299419 11570.17 11570.17 11570.17    11051.2    6380.673    6380.673\n         11549983260 9563701017 3708702676 3708702676 1747135467 1682386860\n35299419    6380.673   6214.692   4000.016   4000.016   4234.058   5702.077\n         128349051 6486730562   691582 41466838 4533088712 4533088770\n35299419  3841.384   581.0582 597.3575 2654.314   4884.169   5081.596\n         5589038074 850508342 4639702744 5589038074 3227275829 1684055602\n35299419   4823.045  375.2084   926.0127   4823.045   3808.049    753.752\n           474557 1584608915 1517984612 11513738570 1684226066 27679037\n35299419 556.5173    713.536   622.6949    619.2467    1225.66 3452.874\n         1381614134 5337216850 11549983251 11549983257 11549983257 11549983252\n35299419   3363.827   5226.252    6439.123      6411.8      6411.8    6415.946\n         11549983252 11549983252 11549983252 11549983251 11549983251\n35299419    6415.946    6415.946    6415.946    6439.123    6439.123\n         11549983252 11549983247 11549983251 11549983251 11549983251\n35299419    6415.946    6453.555    6439.123    6439.123    6439.123\n         11549983251 12034855970 1517209100 1517209100 9445278102 4179720618\n35299419    6439.123    12081.21   9239.889   9239.889   5296.254    9050.93\n         4179720618 360692085 618271588 360692085 4179720618 12033597756\n35299419    9050.93  9002.504  8275.415  9002.504    9050.93    11708.13\n         12033597753 12033597764 12033597762 9307660916 11364252738 1684259012\n35299419    11696.37    11754.26    11722.44   9259.759    3349.922   2300.121\n         4361632708 1314915645 7006050181   474383   474383   474383 1448664912\n35299419   815.5434   3219.572   5485.599 905.8238 905.8238 905.8238   4950.036\n         26658915 118724163 1804412114 107228955 10713423   547019 3754347320\n35299419 4968.988  858.9023   3216.095  3159.418 11347.78 4919.699   1813.109\n         33033074 3080970374 20464883 1517208858 107145004 7028566698\n35299419 8871.589   1666.407 9310.528   9370.483  4462.093   3177.182\n         1314915645 36866188 1682829408 5433229750 1517208858   194076\n35299419   3219.572 3118.604   5756.392   9250.383   9370.483 5462.571\n         130069978 360689836 108044084 108044703 111996556 1684631749 107912783\n35299419  4046.518  8931.139  6193.605  6126.632   8616.91   6647.454  6049.243\n         112032936   210208 1240746711 1917246883   596120 11659717644\n35299419  10483.53 1505.914   4995.748   11232.78 1574.826    5218.511\n         942789065 11527934107 1517209249   402702 26658915 361463301 131956097\n35299419  1804.091    3084.572   8813.936 4512.959 4968.988  3803.913  12902.97\n         11737454087 1381614134 1381614134   474557 3357036324 33032729\n35299419    5250.123   3363.827   3363.827 556.5173   788.7212   9824.4\n         4533088711 5433229750 5433229750 5433229750 4787197864 4787197864\n35299419   4866.125   9250.383   9250.383   9250.383   9232.061   9232.061\n         4787197864 4787197864 5433229750 5433229750 5837229773 6992272607\n35299419   9232.061   9232.061   9250.383   9250.383   5774.382   5383.464\n          402703 361463301 1684581956 1684582104 35298117 849545628 850213112\n35299419 4492.53  3803.913   4661.927   4913.756 1543.604  1510.665  1559.537\n         4533088743 1917247132 5478469295 5478469294   470102 1584776930\n35299419   5081.936   11390.34   4719.045   4723.757 4271.398   3277.902\n         1682809610 11659717644 4533088770 4533088770 4533088770 4533088770\n35299419   5819.778    5218.511   5081.596   5081.596   5081.596   5081.596\n         4533088770 4533088770 4533088770   320774 8501630403 107887646\n35299419   5081.596   5081.596   5081.596 4963.868   6045.425   5140.89\n           402702 1681595494 11549983252 8489626386 1917246879 1917246684\n35299419 4512.959   5601.532    6415.946   4625.997   11303.07   11327.69\n         5589038074 5589038074 5589038074 5589038074 4533088711 4533088714\n35299419   4823.045   4823.045   4823.045   4823.045   4866.125   4895.861\n         4533088711 4533088714 4533088714 4533088713 130240118 110151723\n35299419   4866.125   4895.861   4895.861   4890.115  11885.28  6551.723\n         110151723 110151723 110151723 110151723 110151723 29368594 26658915\n35299419  6551.723  6551.723  6551.723  6551.723  6551.723 4582.555 4968.988\n         7305624459 106007661 8987883618 5859802411   518610   518610   518611\n35299419   11517.33  4548.575   4136.321   1129.672 1302.289 1302.289 1320.727\n         800803720   290950 4081504238 4081504206 4533088742 35510611\n35299419  3344.675 4464.824   1332.334   1122.165   5061.751 11570.17\n         12033597756 6732089007 12036553018 7006091789 4559843598 5049080642\n35299419    11708.13    3002.79    6503.173   5562.273   3135.612   3166.975\n         1917246684 35309619 1740407820 158373125 7006142009 7538876197\n35299419   11327.69 2920.565    1677.06  2312.611   5620.269   5518.542\n         1787929945 7006142009 7006142009 4533088693 851157206 1592759339\n35299419   5520.407   5620.269   5620.269   4733.129  1128.534   8944.679\n          191611 107545686 8788625297 8788625298 8788625298 4081504206\n35299419 2060.98  7498.559   1114.958   1092.273   1092.273   1122.165\n         8788625298 4936929522   691638 2113130392 8788728291 4741418602\n35299419   1092.273    4661.76 2205.285   3361.825   2738.453   3026.335\n         9465974729 4559843597 4559843597 360754572 360754572 1947998799\n35299419   3260.599   3275.482   3275.482  3370.976  3370.976   3400.026\n         107228953 4559843597 360465477   691582 1584777018 7111358956\n35299419  3139.566   3275.482   2487.06 597.3575    3294.96   7368.158\n         6801562238 6801562238 7111358956\n35299419    7329.26    7329.26   7368.158\n\n\nOur output shows the calculations for the first school - and the distances between the school and every fast-food outlet. Because we manually overwrote the values for all one-way streets as well as that we extracted the larges connected graph only, we currently should not have any NA values.\n\n\n\n\n\n\nThe dodgr vignette notes that a distance matrix obtained from running dodgr_distances on graph_connected should generally contain no NA values, although some points may still be effectively unreachable due to one-way connections (or streets). Thus, routing on the largest connected component of a directed graph ought to be expected to yield the minimal number of NA values, which may sometimes be more than zero. Note further that spatial routing points (expressed as from and/or to arguments) will in this case be mapped to the nearest vertices of graph_connected, rather than the potentially closer nearest points of the full graph.\n\n\n\nThe next step of processing all depends on what you are trying to assess. Today we want to understand which schools have a closer proximity to fast-food outlets and which do not, quantified by how many outlets are within walking distance. We will therefore look to count how many outlets are with walking distance from each school and store this as a new column within our ports_school data frame.\n\n\n\nR code\n\n# fastfood outlets within 400m\nports_schools$ff_within_400m &lt;- rowSums(sch_to_ff_calc &lt;= 400)\n\n# fastfood outlets within 800m\nports_schools$ff_within_800m &lt;- rowSums(sch_to_ff_calc &lt;= 800)\n\n# fastfood outlets within 1000m\nports_schools$ff_within_1km &lt;- rowSums(sch_to_ff_calc &lt;= 1000)\n\n\nYou can inspect the ports_schools object to see the results of this analysis.\n\n\n\nNow you have calculated the number of fast-food outlets within specific distances from every school in Portsmouth and should get the idea behind a basic accessibility analysis, your task is to estimate the accessibility of fast-food outlets at the LSOA scale and compare this to the 2019 Index of Multiple Deprivation.\n\n\n\n\n\n\nThis skills and steps required for this analysis are not just based on this week’s practical, but you will have to combine all your knowledge of coding and spatial analysis you have gained over the past weeks.\n\n\n\nOne way of doing this, is by taking some of the following steps:\n\nDownload the 2011 LSOA boundaries and extract only those that relate to Portsmouth.\nDownload the 2019 Index of Multiple Deprivation scores.\nDecide on an accessibility measure, such as:\n\nThe average number of fast-food restaurants within x meters of a school within each LSOA.\nThe average distance a fast-food restaurant is from a school within each LSOA.\nThe (average) shortest distance a fast-food restaurant is from a school within each LSOA.\nThe minimum shortest distance a fast-food outlet is from a school within each LSOA.\n\nAggregate accessibility scores to the LSOA level.\nJoin the 2019 Index of Multiple Deprivation data to your LSOA dataset.\nFor each IMD decile, calculate the average for your chosen aggregate measure and produce a table.\n\nUsing your approach what do you think: are fast-food restaurants, on average, more accessible for students at schools that are located within LSOAs with a lower IMD decile (more deprived) when compared to students at schools that are located within LSOAs with a higher IMD decile (less deprived)?"
  },
  {
    "objectID": "02-network.html#wm-w10",
    "href": "02-network.html#wm-w10",
    "title": "Transport Network Analysis",
    "section": "",
    "text": "We have now conducted some basic accessibility analysis, however, there is some additional fundamental challenges to consider in the context of transport network and accessibility analysis:\n\nHow do the different weight profiles of the dodgr package work? How would one go about creating your own weight profile? How would using a different weight profiles affect the results of your analysis?\nWhy do we have unconnected segments in the extracted transport network? How would you inspect these unconnected segments? Would they need to be connected? If so, how would one do this?\nWhy you think all Origins and Destinations are mapped onto the closest network points? Is this always the best option? What alternative methods could you think of and how would you implement these?\n\n\n\n\n\n\n\nIf you want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R."
  },
  {
    "objectID": "02-network.html#byl-ntx",
    "href": "02-network.html#byl-ntx",
    "title": "Transport Network Analysis",
    "section": "",
    "text": "Having finished this tutorial on transport network analysis and, hopefully, having been able to independently conduct some further area-profiling using IMD deciles, you have now reached the end of this week’s content."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles of Spatial Analysis 2023-2024",
    "section": "",
    "text": "Principles of Spatial Analysis 2023-2024\n\n\n\n\n\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nFoundational Concepts\nSpatial analysis for data science\n\n\n2\nFoundational Concepts\nGraphical representation of spatial data\n\n\n3\nFoundational Concepts\nSpatial autocorrelation\n\n\n4\nRaster data\nSuitability Mapping I\n\n\n5\nRaster data\nSuitability Mapping II\n\n\n\nReading week\nReading week\n\n\n6\nRaster data\nGeostatistics using Kriging\n\n\n7\nApplied Spatial Analysis\nGeodemographics\n\n\n8\nApplied Spatial Analysis\nTransport network analysis\n\n\n9\nSpatial models\nSpatial models I\n\n\n10\nSpatial models\nSpatial models II\n\n\n\n\n\n\n\n\n\nThis GitHub page contains the material for Week 07 (Geodemographics) and Week 08 (Transport network analysis) of Principles of Spatial Analysis 2023-2024. The content for 2022-2023 has been archived and can be found here: [Link]"
  }
]